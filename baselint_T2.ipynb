{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c0f5229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/nlp_server/venv310/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loaded Config ===\n",
      "{'data_path': './data/',\n",
      " 'model_name': 'eenzeenee/t5-base-korean-summarization',\n",
      " 'output_dir': './t5_outputs'}\n",
      "{'bos_token': '',\n",
      " 'decoder_max_len': 80,\n",
      " 'encoder_max_len': 512,\n",
      " 'eos_token': '</s>',\n",
      " 'special_tokens': ['#Person1#',\n",
      "                    '#Person2#',\n",
      "                    '#Person3#',\n",
      "                    '#Person4#',\n",
      "                    '#Person5#',\n",
      "                    '#Person6#',\n",
      "                    '#Person7#',\n",
      "                    '#PhoneNumber#',\n",
      "                    '#Address#',\n",
      "                    '#PassportNumber#'],\n",
      " 't5_prefix': 'summarize: '}\n",
      "{'do_eval': True,\n",
      " 'do_train': True,\n",
      " 'early_stopping_patience': 3,\n",
      " 'early_stopping_threshold': 0.001,\n",
      " 'evaluation_strategy': 'epoch',\n",
      " 'fp16': True,\n",
      " 'generation_max_length': 80,\n",
      " 'gradient_accumulation_steps': 1,\n",
      " 'greater_is_better': True,\n",
      " 'learning_rate': 5e-05,\n",
      " 'load_best_model_at_end': True,\n",
      " 'logging_dir': './logs',\n",
      " 'logging_strategy': 'epoch',\n",
      " 'lr_scheduler_type': 'cosine',\n",
      " 'metric_for_best_model': 'rouge-l',\n",
      " 'num_train_epochs': 10,\n",
      " 'optim': 'adamw_torch',\n",
      " 'overwrite_output_dir': True,\n",
      " 'per_device_eval_batch_size': 16,\n",
      " 'per_device_train_batch_size': 8,\n",
      " 'predict_with_generate': True,\n",
      " 'report_to': 'none',\n",
      " 'save_strategy': 'epoch',\n",
      " 'save_total_limit': 5,\n",
      " 'seed': 42,\n",
      " 'warmup_ratio': 0.1,\n",
      " 'weight_decay': 0.01}\n",
      "{'entity': 'quriquri7', 'name': 't5-base-run1', 'project': 'dialogue-summ-t5'}\n",
      "{'batch_size': 32,\n",
      " 'ckt_path': 'ì¶”ë¡ ì—_ì‚¬ìš©í• _ì²´í¬í¬ì¸íŠ¸_ê²½ë¡œ_ë˜ëŠ”_output_dir',\n",
      " 'early_stopping': True,\n",
      " 'generate_max_length': 80,\n",
      " 'no_repeat_ngram_size': 3,\n",
      " 'num_beams': 5,\n",
      " 'remove_tokens': ['<usr>', '', '</s>', '<pad>'],\n",
      " 'result_path': './prediction_t5/'}\n",
      "\n",
      "=== Train Sample ===\n",
      "             fname                                           dialogue  \\\n",
      "12452  train_12455  #Person1#: ì•ˆë…•í•˜ì„¸ìš”. í˜¹ì‹œ ë§¨ì²´ìŠ¤í„°ì—ì„œ ì˜¤ì‹  Mr. Green ë§ìœ¼ì‹ ê°€ìš”...   \n",
      "12453  train_12456  #Person1#: Mister Ewingì´ ìš°ë¦¬ íšŒì˜ì¥ì— 4ì‹œì— ì˜¤ë¼ê³  í–ˆì§€, ë§...   \n",
      "12454  train_12457  #Person1#: ì˜¤ëŠ˜ ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\\n#Person2#: ì°¨ë¥¼ ë¹Œë¦¬ê³  ì‹¶...   \n",
      "12455  train_12458  #Person1#: ë„ˆ ì˜¤ëŠ˜ ì¢€ ê¸°ë¶„ ì•ˆ ì¢‹ì•„ ë³´ì¸ë‹¤? ë¬´ìŠ¨ ì¼ ìˆì–´?\\n#Pers...   \n",
      "12456  train_12459  #Person1#: ì—„ë§ˆ, ë‚˜ ë‹¤ìŒ ì£¼ í† ìš”ì¼ì— ì´ëª¨ë¶€ë„¤ ê°€ì¡± ë³´ëŸ¬ ê°€ëŠ”ë°, ì˜¤ëŠ˜ ...   \n",
      "\n",
      "                                                 summary     topic  \n",
      "12452  Tan Lingì€ í°ë¨¸ë¦¬ì™€ ìˆ˜ì—¼ì´ íŠ¹ì§•ì¸ Mr. Greenì„ ë§ì´í•˜ì—¬ í˜¸í…”ë¡œ ì•ˆë‚´í•©...     í˜¸í…” ì•ˆë‚´  \n",
      "12453  #Person1#ê³¼ #Person2#ëŠ” Mister Ewingì˜ ìš”ì²­ì— ë”°ë¼ íšŒì˜ì¥...     íšŒì˜ ì¤€ë¹„  \n",
      "12454       #Person2#ëŠ” #Person1#ì˜ ë„ì›€ìœ¼ë¡œ 5ì¼ ë™ì•ˆ ì†Œí˜•ì°¨ë¥¼ ëŒ€ì—¬í•©ë‹ˆë‹¤.     ì°¨ëŸ‰ ëŒ€ì—¬  \n",
      "12455  #Person2#ì˜ ì–´ë¨¸ë‹ˆê°€ ì§ì¥ì„ ìƒìœ¼ì…¨ë‹¤. #Person2#ëŠ” ì–´ë¨¸ë‹ˆê°€ ìš°ìš¸í•´í•˜...    ì‹¤ì§ê³¼ ëŒ€ì²˜  \n",
      "12456  #Person1#ì€ ë‹¤ìŒ ì£¼ í† ìš”ì¼ì— ì´ëª¨ë¶€ë„¤ ê°€ì¡±ì„ ë°©ë¬¸í•˜ê¸° ìœ„í•´ ì§ì„ ì‹¸ì•¼ í•˜ëŠ”...  ê°€ì¡± ë°©ë¬¸ ì¤€ë¹„  \n",
      "\n",
      "=== Dev Sample ===\n",
      "       fname                                           dialogue  \\\n",
      "494  dev_495  #Person1#: ìƒˆí•´ê°€ ë˜ë‹ˆê¹Œ ë‚˜ë„ ìƒˆ ì¶œë°œì„ í•˜ê¸°ë¡œ í–ˆì–´.\\n#Person2#...   \n",
      "495  dev_496  #Person1#: ë„ˆ Joeë‘ ê²°í˜¼í–ˆì§€?\\n#Person2#: Joe? ë¬´ìŠ¨ ë§ì´...   \n",
      "496  dev_497  #Person1#: ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”, ì•„ì¤Œë§ˆ?\\n#Person2#: ì œ ì°¨ì—ì„œ ...   \n",
      "497  dev_498  #Person1#: ì—¬ë³´ì„¸ìš”, ì•„ë§ˆì¡´ ê³ ê° ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\\n#...   \n",
      "498  dev_499  #Person1#: ë²Œì¨ ì—¬ë¦„ì´ ë‹¤ê°€ì˜¤ë‹¤ë‹ˆ ë¯¿ê¸°ì§€ ì•Šì•„. \\n#Person2#: ë§...   \n",
      "\n",
      "                                               summary       topic  \n",
      "494  #Person1#ì€ ìƒˆí•´ì— ë‹´ë°°ë¥¼ ëŠê³  ì»¤ë°ì•„ì›ƒ í•˜ê¸°ë¡œ ê²°ì‹¬í–ˆìŠµë‹ˆë‹¤. #Person...       ìƒˆí•´ ê²°ì‹¬  \n",
      "495  #Person1#ì€ #Person2#ê°€ Joeì™€ ê²°í˜¼í–ˆë‹¤ê³  ìƒê°í•˜ì§€ë§Œ, #Perso...   ì‚¬ë‘ê³¼ ê²°í˜¼ ì˜¤í•´  \n",
      "496  #Person2#ì˜ ì°¨ì—ì„œ ì†Œë¦¬ê°€ ë‚˜ë©°, ë¸Œë ˆì´í¬ ìˆ˜ë¦¬ê°€ í•„ìš”í•œ ìƒí™©ì…ë‹ˆë‹¤. #Pe...  ì°¨ëŸ‰ ì†ŒìŒ ë° ìˆ˜ë¦¬  \n",
      "497  #Person2#ê°€ ì•„ë§ˆì¡´ ê³ ê° ì„œë¹„ìŠ¤ì— ì „í™”í•˜ì—¬ ì•„ë§ˆì¡´ì—ì„œ êµ¬ë§¤í•œ ì±…ì— 53í˜ì´ì§€...    ì±… í˜ì´ì§€ ëˆ„ë½  \n",
      "498  #Person2#ëŠ” ì—¬ë¦„ë°©í•™ ë™ì•ˆ íŒŒí‹°ì—ì„œ ì¼í•˜ëŠ” íšŒì‚¬ì—ì„œ ì¼í•˜ë©°, ì£¼ë¡œ ìŒì‹ ì¤€ë¹„...    ì—¬ë¦„ë°©í•™ ì¼ìë¦¬  \n"
     ]
    }
   ],
   "source": [
    "# 1. ë°ì´í„° ë° í™˜ê²½ì„¤ì • (T5 ê¸°ë°˜ Dialogue Summarization)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl  # ì“°ì§€ ì•Šë”ë¼ë„ requirements ë§ì¶”ê¸°ìš©\n",
    "from rouge import Rouge\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "import wandb  # ì„ íƒ: í•™ìŠµ ë¡œê¹…ìš©\n",
    "\n",
    "# =========================================\n",
    "# 1-1. ê¸°ë³¸ ì„¤ì •: T5 í•œêµ­ì–´ ìš”ì•½ ëª¨ë¸ ì„ íƒ\n",
    "# =========================================\n",
    "MODEL_NAME = \"eenzeenee/t5-base-korean-summarization\"  # T5-base í•œêµ­ì–´ ìš”ì•½ ëª¨ë¸\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# T5 ê³„ì—´ì€ ë³´í†µ bos_tokenì´ ì—†ìœ¼ë¯€ë¡œ None ë°©ì§€\n",
    "bos_token = tokenizer.bos_token if tokenizer.bos_token is not None else \"\"  # decoder input ì•ì— ë¶™ì¼ í† í° (ì—†ì–´ë„ ë¨)\n",
    "eos_token = tokenizer.eos_token if tokenizer.eos_token is not None else \"\"  # ë³´í†µ \"</s>\"\n",
    "\n",
    "# =========================================\n",
    "# 1-2. YAML Config êµ¬ì„±\n",
    "# =========================================\n",
    "config_data = {\n",
    "    \"general\": {\n",
    "        \"data_path\": \"./data/\",           # train.csv, dev.csv, test.csv ìœ„ì¹˜\n",
    "        \"model_name\": MODEL_NAME,          # ì‚¬ìš©í•  T5 ëª¨ë¸\n",
    "        \"output_dir\": \"./t5_outputs\",      # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"encoder_max_len\": 512,            # ëŒ€í™”ê°€ ê½¤ ê¸¸ ìˆ˜ ìˆì–´ì„œ 512 ìœ ì§€\n",
    "        \"decoder_max_len\": 80,             # ìš”ì•½ì€ ìƒëŒ€ì ìœ¼ë¡œ ì§§ê²Œ\n",
    "        \"bos_token\": bos_token,\n",
    "        \"eos_token\": eos_token,\n",
    "        # ëŒ€í™” ë‚´ íŠ¹ìˆ˜ í† í°ì„ ë³´ì¡´í•˜ê¸° ìœ„í•´ special_tokens ë“±ë¡\n",
    "        \"special_tokens\": [\n",
    "            \"#Person1#\", \"#Person2#\", \"#Person3#\",\n",
    "            \"#Person4#\", \"#Person5#\", \"#Person6#\", \"#Person7#\",\n",
    "            \"#PhoneNumber#\", \"#Address#\", \"#PassportNumber#\"\n",
    "        ],\n",
    "        # T5 ìš”ì•½ í”„ë¡¬í”„íŠ¸\n",
    "        \"t5_prefix\": \"summarize: \"\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": 10,            # KoBART 20ep â†’ T5 8~12ep ê¶Œì¥, ì¼ë‹¨ 10\n",
    "        \"learning_rate\": 5e-5,             # ìš”ì•½ íƒœìŠ¤í¬ìš©ìœ¼ë¡œ ìì£¼ ì“°ëŠ” lr\n",
    "        \"per_device_train_batch_size\": 8,  # OOM ë°©ì§€ & ì•ˆì •ì ì¸ í•™ìŠµ\n",
    "        \"per_device_eval_batch_size\": 16,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr_scheduler_type\": \"cosine\",\n",
    "        \"optim\": \"adamw_torch\",\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"save_total_limit\": 5,\n",
    "        \"fp16\": True,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"seed\": 42,\n",
    "        \"logging_dir\": \"./logs\",\n",
    "        \"logging_strategy\": \"epoch\",\n",
    "        \"predict_with_generate\": True,     # ROUGE ê³„ì‚°ì„ ìœ„í•´ generate ì‚¬ìš©\n",
    "        \"generation_max_length\": 80,\n",
    "        \"metric_for_best_model\": \"rouge-l\",  # ROUGE-L ê¸°ì¤€ìœ¼ë¡œ best ckpt ì„ íƒ\n",
    "        \"greater_is_better\": True,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_threshold\": 0.001,\n",
    "        \"report_to\": \"none\",             # \"wandb\" ë˜ëŠ” \"none\"\n",
    "    },\n",
    "    \"wandb\": {\n",
    "        \"entity\": \"quriquri7\",    # ì˜ˆ: \"quriquri7\"\n",
    "        \"project\": \"dialogue-summ-t5\",     # ì›í•˜ëŠ” í”„ë¡œì íŠ¸ëª…\n",
    "        \"name\": \"t5-base-run1\"            # run ì´ë¦„\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"ckt_path\": \"ì¶”ë¡ ì—_ì‚¬ìš©í• _ì²´í¬í¬ì¸íŠ¸_ê²½ë¡œ_ë˜ëŠ”_output_dir\",  # ì˜ˆ: \"t5_outputs/checkpoint-xxxx\"\n",
    "        \"result_path\": \"./prediction_t5/\",\n",
    "        \"no_repeat_ngram_size\": 3,        # ë°˜ë³µ ê°ì†Œ â†’ ROUGE ìƒìŠ¹ì— ë„ì›€\n",
    "        \"early_stopping\": True,\n",
    "        \"generate_max_length\": 80,\n",
    "        \"num_beams\": 5,                   # ë¹” ì„œì¹˜; 4~8 ë²”ìœ„ íŠœë‹ ì¶”ì²œ\n",
    "        \"batch_size\": 32,\n",
    "        \"remove_tokens\": [\n",
    "            \"<usr>\",\n",
    "            bos_token,\n",
    "            eos_token,\n",
    "            tokenizer.pad_token if tokenizer.pad_token is not None else \"\",\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "# =========================================\n",
    "# 1-3. Config ì €ì¥ ë° ë¡œë“œ\n",
    "# =========================================\n",
    "os.makedirs(\"./\", exist_ok=True)\n",
    "config_path = \"./config_t5.yaml\"\n",
    "\n",
    "with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.dump(config_data, f, allow_unicode=True)\n",
    "\n",
    "with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    loaded_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"=== Loaded Config ===\")\n",
    "pprint(loaded_config[\"general\"])\n",
    "pprint(loaded_config[\"tokenizer\"])\n",
    "pprint(loaded_config[\"training\"])\n",
    "pprint(loaded_config[\"wandb\"])\n",
    "pprint(loaded_config[\"inference\"])\n",
    "\n",
    "# =========================================\n",
    "# 1-4. ë°ì´í„° ë¡œë“œ í™•ì¸\n",
    "# =========================================\n",
    "data_path = loaded_config[\"general\"][\"data_path\"]\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "val_df   = pd.read_csv(os.path.join(data_path, \"dev.csv\"))\n",
    "\n",
    "print(\"\\n=== Train Sample ===\")\n",
    "print(train_df.tail())\n",
    "print(\"\\n=== Dev Sample ===\")\n",
    "print(val_df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "416b59dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/nlp_server/venv310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12457, 499)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. ë°ì´í„° ê°€ê³µ ë° Dataset í´ë˜ìŠ¤ (T5 ì •ì„ íŒ¨í„´)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DialogTrainDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, encoder_max_len, decoder_max_len, prefix=\"summarize: \"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder_max_len = encoder_max_len\n",
    "        self.decoder_max_len = decoder_max_len\n",
    "        self.prefix = prefix\n",
    "\n",
    "        self.dialogues = df[\"dialogue\"].tolist()\n",
    "        self.summaries = df[\"summary\"].tolist()\n",
    "\n",
    "        # ë¯¸ë¦¬ í† í°í™” í•´ë‘ê¸° (ë©”ëª¨ë¦¬ ì—¬ìœ  ìˆìœ¼ë‹ˆ ê´œì°®ìŒ)\n",
    "        encodings = tokenizer(\n",
    "            [self.prefix + d for d in self.dialogues],\n",
    "            max_length=self.encoder_max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # labelìš© summary í† í°í™”\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                self.summaries,\n",
    "                max_length=self.decoder_max_len,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )[\"input_ids\"]\n",
    "\n",
    "        # pad â†’ -100 (lossì—ì„œ ë¬´ì‹œ)\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        self.input_ids = encodings[\"input_ids\"]\n",
    "        self.attention_mask = encodings[\"attention_mask\"]\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "class DialogTestDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, encoder_max_len, prefix=\"summarize: \"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder_max_len = encoder_max_len\n",
    "        self.prefix = prefix\n",
    "\n",
    "        self.fnames = df[\"fname\"].tolist()\n",
    "        self.dialogues = df[\"dialogue\"].tolist()\n",
    "\n",
    "        encodings = tokenizer(\n",
    "            [self.prefix + d for d in self.dialogues],\n",
    "            max_length=self.encoder_max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        self.input_ids = encodings[\"input_ids\"]\n",
    "        self.attention_mask = encodings[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"fname\": self.fnames[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "# train / val Dataset ìƒì„±\n",
    "encoder_max_len = loaded_config[\"tokenizer\"][\"encoder_max_len\"]\n",
    "decoder_max_len = loaded_config[\"tokenizer\"][\"decoder_max_len\"]\n",
    "t5_prefix = loaded_config[\"tokenizer\"][\"t5_prefix\"]\n",
    "\n",
    "train_dataset = DialogTrainDataset(\n",
    "    train_df,\n",
    "    tokenizer,\n",
    "    encoder_max_len=encoder_max_len,\n",
    "    decoder_max_len=decoder_max_len,\n",
    "    prefix=t5_prefix,\n",
    ")\n",
    "\n",
    "val_dataset = DialogTrainDataset(\n",
    "    val_df,\n",
    "    tokenizer,\n",
    "    encoder_max_len=encoder_max_len,\n",
    "    decoder_max_len=decoder_max_len,\n",
    "    prefix=t5_prefix,\n",
    ")\n",
    "\n",
    "len(train_dataset), len(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "470b0405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/nlp_server/venv310/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/nlp_server/venv310/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# 3. Trainer ë° TrainingArguments (T5 + DataCollator, wandb ì—†ìŒ)\n",
    "\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics_t5(pred):\n",
    "    \"\"\"\n",
    "    Trainerê°€ generateí•œ ìš”ì•½ vs gold summary(labels)ë¥¼ ROUGEë¡œ í‰ê°€\n",
    "    \"\"\"\n",
    "    rouge = Rouge()\n",
    "\n",
    "    preds = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    # -100 â†’ pad_token_idë¡œ ë³µêµ¬í•´ì„œ ë””ì½”ë”©\n",
    "    preds[preds == -100] = tokenizer.pad_token_id\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # ë””ë²„ê¹…ìš© ì¼ë¶€ ì¶œë ¥\n",
    "    print(\"-\" * 150)\n",
    "    for i in range(3):\n",
    "        if i >= len(decoded_preds):\n",
    "            break\n",
    "        print(f\"[PRED {i}]: {decoded_preds[i][:150]}\")\n",
    "        print(f\"[GOLD {i}]: {decoded_labels[i][:150]}\")\n",
    "        print(\"-\" * 150)\n",
    "\n",
    "    # ROUGE ê³„ì‚°\n",
    "    scores = rouge.get_scores(decoded_preds, decoded_labels, avg=True)\n",
    "    result = {k: v[\"f\"] for k, v in scores.items()}  # F1ë§Œ ì‚¬ìš©\n",
    "\n",
    "    # Trainer ë¡œê·¸ìš©ìœ¼ë¡œ numpy ë³€í™˜\n",
    "    return {k: float(v) for k, v in result.items()}\n",
    "\n",
    "\n",
    "def load_t5_model(config, device):\n",
    "    model_name = config[\"general\"][\"model_name\"]\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # special_tokens ì¶”ê°€ (ë°œí™”ì/ê°œì¸ì •ë³´ í† í°)\n",
    "    special_tokens_dict = {\"additional_special_tokens\": config[\"tokenizer\"][\"special_tokens\"]}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "from transformers import set_seed\n",
    "set_seed(loaded_config[\"training\"][\"seed\"])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = load_t5_model(loaded_config, device)\n",
    "\n",
    "# DataCollator - padding & labels ì •ë¦¬ ìë™ ì²˜ë¦¬\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=loaded_config[\"general\"][\"output_dir\"],\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,              # ì¼ë‹¨ 5epoch ì •ë„ë¡œ ì‹œì‘\n",
    "    learning_rate=3e-5,              # ì¡°ê¸ˆ ë‚®ê²Œ\n",
    "    per_device_train_batch_size=4,   # ë©”ëª¨ë¦¬ ì—¬ìœ ì— ë”°ë¼ ì¡°ì •\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch\",\n",
    "    gradient_accumulation_steps=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=loaded_config[\"training\"][\"seed\"],\n",
    "    logging_dir=loaded_config[\"training\"][\"logging_dir\"],\n",
    "    logging_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=loaded_config[\"training\"][\"generation_max_length\"],\n",
    "    generation_num_beams=loaded_config[\"inference\"][\"num_beams\"],\n",
    "    metric_for_best_model=\"rouge-l\",\n",
    "    greater_is_better=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",               # ğŸ”¥ wandb ì™„ì „ ë¹„í™œì„±í™”\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_t5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66d3e315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15575' max='15575' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15575/15575 48:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.089900</td>\n",
       "      <td>0.860992</td>\n",
       "      <td>0.271599</td>\n",
       "      <td>0.103334</td>\n",
       "      <td>0.260741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.849400</td>\n",
       "      <td>0.822655</td>\n",
       "      <td>0.274060</td>\n",
       "      <td>0.101674</td>\n",
       "      <td>0.259277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.761300</td>\n",
       "      <td>0.817566</td>\n",
       "      <td>0.283521</td>\n",
       "      <td>0.104598</td>\n",
       "      <td>0.268993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.706600</td>\n",
       "      <td>0.816271</td>\n",
       "      <td>0.290288</td>\n",
       "      <td>0.109769</td>\n",
       "      <td>0.275376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.681600</td>\n",
       "      <td>0.820255</td>\n",
       "      <td>0.289509</td>\n",
       "      <td>0.109579</td>\n",
       "      <td>0.275468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 0]: #Person1#ì€ #Person2#ì—ê²Œ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ì¡°ì–¸í•©ë‹ˆë‹¤.\n",
      "[GOLD 0]: #Person2#ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2#ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 1]: #Person1#ê³¼ JimmyëŠ” 3ì‹œ 30ë¶„ì— ì²´ìœ¡ê´€ì—ì„œ ìš´ë™ì„ í•˜ê¸°ë¡œ ê²°ì •í•©ë‹ˆë‹¤.\n",
      "[GOLD 1]: #Person1#ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 2]: #Person2#ëŠ” #Person1#ì—ê²Œ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ìœ¼ë¼ê³  ì¡°ì–¸í•©ë‹ˆë‹¤.\n",
      "[GOLD 2]: #Person1#ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2#ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1#ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 0]: #Person2#ëŠ” ìˆ¨ì‰¬ê¸°ê°€ ì–´ë µê³  ì•Œë ˆë¥´ê¸° ì¦ìƒì´ ìˆìŠµë‹ˆë‹¤. #Person1#ì€ #Person2#ì—ê²Œ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ì¡°ì–¸í•©ë‹ˆë‹¤.\n",
      "[GOLD 0]: #Person2#ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2#ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 1]: #Person1#ì€ #Person2#ì—ê²Œ ìš´ë™ì„ ì œì•ˆí•˜ì§€ë§Œ, #Person2#ëŠ” ë†êµ¬ ë•Œë¬¸ì— ë‹¤ë¦¬ê°€ ì•„í”„ë‹¤ê³  í•©ë‹ˆë‹¤. ê²°êµ­ #Person1#ì€ #Person2#ë¥¼ ì²´ìœ¡ê´€ì—ì„œ ë§Œë‚˜ê¸°ë¡œ í•©ë‹ˆë‹¤.\n",
      "[GOLD 1]: #Person1#ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 2]: #Person2#ëŠ” #Person1#ì—ê²Œ ê±´ê°•ì„ ìœ„í•´ ê³¼ì¼, ì±„ì†Œ, ë‹­ê³ ê¸°ë¥¼ ë¨¹ëŠ”ë‹¤ê³  ë§í•©ë‹ˆë‹¤. #Person2#ëŠ” ê³¼ì¼ê³¼ ì±„ì†Œê°€ ê±´ê°•ì— ì¢‹ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\n",
      "[GOLD 2]: #Person1#ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2#ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1#ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 0]: #Person2#ëŠ” ìˆ¨ì‰¬ê¸°ê°€ í˜ë“¤ë‹¤ê³  #Person1#ì—ê²Œ ë§í•©ë‹ˆë‹¤. #Person1#ì€ #Person2#ì—ê²Œ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ì¡°ì–¸í•©ë‹ˆë‹¤.\n",
      "[GOLD 0]: #Person2#ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2#ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 1]: #Person1#ì€ #Person2#ì—ê²Œ ë‹¤ë¦¬ì™€ íŒ” ìš´ë™ì„ í•˜ìê³  ì œì•ˆí•˜ì§€ë§Œ, #Person2#ëŠ” ì£¼ê°„ ì¼ì • ë•Œë¬¸ì— ê±°ì ˆí•©ë‹ˆë‹¤. ê²°êµ­ #Person1#ì€ 3ì‹œ 30ë¶„ì— ì²´ìœ¡ê´€ì—ì„œ ë§Œë‚˜ê¸°ë¡œ í•©ë‹ˆë‹¤.\n",
      "[GOLD 1]: #Person1#ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 2]: #Person2#ëŠ” #Person1#ì—ê²Œ ê±´ê°•ì„ ìœ„í•´ ê³¼ì¼ê³¼ ì±„ì†Œ, ë‹­ê³ ê¸°ë¥¼ ë¨¹ëŠ”ë‹¤ê³  ë§í•©ë‹ˆë‹¤. #Person1#ì€ #Person2#ê°€ ë” ê±´ê°•í•´ ë³´ì¸ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\n",
      "[GOLD 2]: #Person1#ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2#ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1#ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 0]: #Person1#ì€ #Person2#ì—ê²Œ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ì œì•ˆí•©ë‹ˆë‹¤.\n",
      "[GOLD 0]: #Person2#ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2#ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 1]: #Person1#ì€ #Person2#ì—ê²Œ ìš´ë™ì„ ì œì•ˆí•˜ê³ , #Person2#ëŠ” ë‹¤ë¦¬ì™€ íŒ” ìš´ë™ì„ í•˜ê¸°ë¡œ í•©ë‹ˆë‹¤. #Person1#ì€ ì£¼ê°„ ì¼ì •ì´ ìˆì–´ #Person2#ëŠ” ê¸ˆìš”ì¼ì— í•˜ê¸°ë¡œ í•©ë‹ˆë‹¤.\n",
      "[GOLD 1]: #Person1#ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 2]: #Person2#ëŠ” #Person1#ì—ê²Œ ê±´ê°•ì„ ìœ„í•´ ì£¼ë¡œ ê³¼ì¼ê³¼ ì±„ì†Œ, ë‹­ê³ ê¸°ë¥¼ ë¨¹ëŠ”ë‹¤ê³  ë§í•©ë‹ˆë‹¤. #Person1#ì€ ê·¸ê²ƒì´ ë” ê±´ê°•í•´ ë³´ì¸ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\n",
      "[GOLD 2]: #Person1#ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2#ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1#ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 0]: #Person2#ëŠ” ìˆ¨ì‰¬ê¸°ê°€ ì–´ë µê³  ì•Œë ˆë¥´ê¸° ì¦ìƒì´ ìˆìŠµë‹ˆë‹¤. #Person1#ì€ #Person2#ì—ê²Œ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ì œì•ˆí•©ë‹ˆë‹¤.\n",
      "[GOLD 0]: #Person2#ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2#ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 1]: #Person1#ì€ #Person2#ì—ê²Œ ìš´ë™ì„ ì œì•ˆí•˜ê³ , #Person2#ëŠ” ë‹¤ë¦¬ì™€ íŒ” ìš´ë™ì„ í•˜ê¸°ë¡œ í•©ë‹ˆë‹¤. #Person1#ì€ ì£¼ê°„ ì¼ì •ì´ ìˆì–´ #Person2#ëŠ” ê¸ˆìš”ì¼ì— í•˜ê¸°ë¡œ í•©ë‹ˆë‹¤.\n",
      "[GOLD 1]: #Person1#ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 2]: #Person2#ëŠ” #Person1#ì—ê²Œ ê±´ê°•ì„ ìœ„í•´ ì£¼ë¡œ ê³¼ì¼ê³¼ ì±„ì†Œ, ë‹­ê³ ê¸°ë¥¼ ë¨¹ëŠ”ë‹¤ê³  ë§í•©ë‹ˆë‹¤. #Person1#ì€ ê·¸ê²ƒì´ ë” ê±´ê°•í•´ ë³´ì¸ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\n",
      "[GOLD 2]: #Person1#ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2#ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1#ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í•™ìŠµ ì™„ë£Œ\n",
      "\n",
      "[DEV ìƒ˜í”Œ ìš”ì•½ í…ŒìŠ¤íŠ¸]\n",
      "GEN : ì–‘ ì”¨ëŠ” ìˆ¨ì‰¬ê¸°ê°€ ì–´ë µê³  ê°€ìŠ´ì´ ë‹µë‹µí•œ ì¦ìƒì„ í˜¸ì†Œí•©ë‹ˆë‹¤. ë‘ ì‚¬ëŒì€ ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ë¼ê³  ì œì•ˆí•©ë‹ˆë‹¤.\n",
      "GOLD: #Person2#ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2#ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# 4. ëª¨ë¸ í•™ìŠµí•˜ê¸° (T5 ì •ì„ íŒŒì´í”„ë¼ì¸)\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(\"âœ… í•™ìŠµ ì™„ë£Œ\")\n",
    "\n",
    "# best model ë¡œë“œë˜ì–´ ìˆìŒ (load_best_model_at_end=True)\n",
    "\n",
    "# devì—ì„œ ìƒ˜í”Œ í•˜ë‚˜ ì§ì ‘ ìš”ì•½í•´ë³´ê¸°\n",
    "sample = val_df.iloc[0]\n",
    "text = loaded_config[\"tokenizer\"][\"t5_prefix\"] + sample[\"dialogue\"]\n",
    "\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=loaded_config[\"tokenizer\"][\"encoder_max_len\"],\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=loaded_config[\"training\"][\"generation_max_length\"],\n",
    "        num_beams=loaded_config[\"inference\"][\"num_beams\"],\n",
    "        no_repeat_ngram_size=3,\n",
    "    )\n",
    "\n",
    "gen = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"\\n[DEV ìƒ˜í”Œ ìš”ì•½ í…ŒìŠ¤íŠ¸]\")\n",
    "print(\"GEN :\", gen)\n",
    "print(\"GOLD:\", sample[\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd84c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- device : cuda ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/nlp_server/venv310/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Inference model & tokenizer loaded ----------\n",
      "test ë°ì´í„° ê°œìˆ˜: 499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:13<00:00,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Inference ì™„ë£Œ, íŒŒì¼ ì €ì¥: ./prediction_t5/output_t5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>ì–‘ ì”¨ëŠ” Ms. Dawsonì—ê²Œ ëª¨ë“  ì§ì›ë“¤ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¥¼ ë³´ë‚´ë‹¬ë¼ê³  ìš”ì²­í•©ë‹ˆë‹¤....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>ì–‘ ì”¨ëŠ” êµí†µì²´ì¦ìœ¼ë¡œ ì¸í•´ ì§‘ì— ê°ˆ ë•Œ ë‹¤ë¥¸ ê¸¸ì„ ì°¾ëŠ” ê²ƒì„ ì œì•ˆí•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>KateëŠ” Mashaì™€ Heroê°€ ë‘ ë‹¬ ë™ì•ˆ ë³„ê±° ëì— ì´í˜¼ ì‹ ì²­ì„ í–ˆë‹¤ê³  ë§í•œë‹¤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>Brianì€ Annaì˜ ìƒì¼ì„ ì¶•í•˜í•˜ë©° íŒŒí‹°ì—ì„œ í•¨ê»˜ ì¶¤ì„ ì¶”ìê³  ì œì•ˆí•œë‹¤.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>ì–‘ ì”¨ì™€ ì°¨ ì”¨ëŠ” ì˜¬ë¦¼í”½ ê³µì›ì˜ ê·œëª¨ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê³  ìˆë‹¤.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fname                                            summary\n",
       "0  test_0  ì–‘ ì”¨ëŠ” Ms. Dawsonì—ê²Œ ëª¨ë“  ì§ì›ë“¤ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¥¼ ë³´ë‚´ë‹¬ë¼ê³  ìš”ì²­í•©ë‹ˆë‹¤....\n",
       "1  test_1  ì–‘ ì”¨ëŠ” êµí†µì²´ì¦ìœ¼ë¡œ ì¸í•´ ì§‘ì— ê°ˆ ë•Œ ë‹¤ë¥¸ ê¸¸ì„ ì°¾ëŠ” ê²ƒì„ ì œì•ˆí•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‘...\n",
       "2  test_2  KateëŠ” Mashaì™€ Heroê°€ ë‘ ë‹¬ ë™ì•ˆ ë³„ê±° ëì— ì´í˜¼ ì‹ ì²­ì„ í–ˆë‹¤ê³  ë§í•œë‹¤...\n",
       "3  test_3         Brianì€ Annaì˜ ìƒì¼ì„ ì¶•í•˜í•˜ë©° íŒŒí‹°ì—ì„œ í•¨ê»˜ ì¶¤ì„ ì¶”ìê³  ì œì•ˆí•œë‹¤.\n",
       "4  test_4                 ì–‘ ì”¨ì™€ ì°¨ ì”¨ëŠ” ì˜¬ë¦¼í”½ ê³µì›ì˜ ê·œëª¨ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê³  ìˆë‹¤."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. ëª¨ë¸ ì¶”ë¡ í•˜ê¸° (T5 + DialogTestDataset ë²„ì „)\n",
    "\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"-\" * 10, f\"device : {device}\", \"-\" * 10)\n",
    "\n",
    "# 5-1. ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì„¤ì • (ì´ë¯¸ í•™ìŠµí•œ best ckptë¡œ êµì²´)\n",
    "loaded_config[\"inference\"][\"ckt_path\"] = \"t5_outputs/checkpoint-12460\"  # ì‹¤ì œ best ckpt ê²½ë¡œ\n",
    "ckpt_path = loaded_config[\"inference\"][\"ckt_path\"]\n",
    "\n",
    "# 5-2. í† í¬ë‚˜ì´ì € & ëª¨ë¸ ë¡œë“œ (í›ˆë ¨ ë•Œì™€ ë™ì¼í•˜ê²Œ special_tokens ì¶”ê°€)\n",
    "model_name = loaded_config[\"general\"][\"model_name\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "special_tokens_dict = {\"additional_special_tokens\": loaded_config[\"tokenizer\"][\"special_tokens\"]}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(ckpt_path)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"-\" * 10, \"Inference model & tokenizer loaded\", \"-\" * 10)\n",
    "\n",
    "# 5-3. test.csv ë¡œë“œ & DialogTestDataset ìƒì„±\n",
    "test_df = pd.read_csv(os.path.join(loaded_config[\"general\"][\"data_path\"], \"test.csv\"))\n",
    "\n",
    "encoder_max_len = loaded_config[\"tokenizer\"][\"encoder_max_len\"]\n",
    "t5_prefix = loaded_config[\"tokenizer\"][\"t5_prefix\"]\n",
    "\n",
    "# ğŸ‘‰ 2ë²ˆ ì…€ì—ì„œ ì •ì˜í•œ DialogTestDataset ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "test_dataset = DialogTestDataset(\n",
    "    test_df,\n",
    "    tokenizer,\n",
    "    encoder_max_len=encoder_max_len,\n",
    "    prefix=t5_prefix,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=loaded_config[\"inference\"][\"batch_size\"],\n",
    ")\n",
    "\n",
    "print(f\"test ë°ì´í„° ê°œìˆ˜: {len(test_dataset)}\")\n",
    "\n",
    "# 5-4. generateë¡œ ìš”ì•½ ìƒì„±\n",
    "all_fnames = []\n",
    "all_summaries = []\n",
    "\n",
    "no_repeat_ngram_size = loaded_config[\"inference\"][\"no_repeat_ngram_size\"]\n",
    "early_stopping = loaded_config[\"inference\"][\"early_stopping\"]\n",
    "max_length = loaded_config[\"inference\"][\"generate_max_length\"]\n",
    "num_beams = loaded_config[\"inference\"][\"num_beams\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        fnames = batch[\"fname\"]\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            early_stopping=early_stopping,\n",
    "        )\n",
    "\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        all_fnames.extend(fnames)\n",
    "        all_summaries.extend(decoded)\n",
    "\n",
    "# 5-5. í•„ìš”í•˜ë©´ ë…¸ì´ì¦ˆ í† í° ì œê±° (ì§€ê¸ˆì€ skip_special_tokens=Trueë¼ í¬ê²Œ í•„ìš” ì—†ê¸´ í•¨)\n",
    "# remove_tokens = loaded_config[\"inference\"][\"remove_tokens\"]\n",
    "# cleaned_summaries = all_summaries.copy()\n",
    "# for token in remove_tokens:\n",
    "#     if token:\n",
    "#         cleaned_summaries = [s.replace(token, \" \") for s in cleaned_summaries]\n",
    "\n",
    "# 5-6. ì œì¶œìš© CSV ì €ì¥\n",
    "output = pd.DataFrame(\n",
    "    {\n",
    "        \"fname\": all_fnames,\n",
    "        \"summary\": all_summaries,  # í•„ìš”í•˜ë©´ ìœ„ì—ì„œ cleaned_summariesë¡œ êµì²´\n",
    "    }\n",
    ")\n",
    "\n",
    "result_path = loaded_config[\"inference\"][\"result_path\"]\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(result_path, \"output_t5.csv\")\n",
    "output.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"âœ… Inference ì™„ë£Œ, íŒŒì¼ ì €ì¥: {save_path}\")\n",
    "output.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
