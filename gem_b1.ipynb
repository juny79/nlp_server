{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b921884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/nlp_server/venv310/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/nlp_server/venv310/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "/data/ephemeral/home/nlp_server/venv310/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training Start...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3120' max='5850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3120/5850 16:14 < 14:13, 3.20 it/s, Epoch 8/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "      <th>Combined Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.613800</td>\n",
       "      <td>0.601240</td>\n",
       "      <td>0.221584</td>\n",
       "      <td>0.067560</td>\n",
       "      <td>0.206402</td>\n",
       "      <td>0.165182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.446000</td>\n",
       "      <td>0.529328</td>\n",
       "      <td>0.275691</td>\n",
       "      <td>0.101218</td>\n",
       "      <td>0.255237</td>\n",
       "      <td>0.210715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.390900</td>\n",
       "      <td>0.516457</td>\n",
       "      <td>0.279172</td>\n",
       "      <td>0.107771</td>\n",
       "      <td>0.261388</td>\n",
       "      <td>0.216110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>0.515866</td>\n",
       "      <td>0.283462</td>\n",
       "      <td>0.111927</td>\n",
       "      <td>0.261767</td>\n",
       "      <td>0.219052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.318700</td>\n",
       "      <td>0.520485</td>\n",
       "      <td>0.288545</td>\n",
       "      <td>0.113943</td>\n",
       "      <td>0.266357</td>\n",
       "      <td>0.222948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.291500</td>\n",
       "      <td>0.526989</td>\n",
       "      <td>0.276672</td>\n",
       "      <td>0.106311</td>\n",
       "      <td>0.255669</td>\n",
       "      <td>0.212884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>0.533678</td>\n",
       "      <td>0.284768</td>\n",
       "      <td>0.112749</td>\n",
       "      <td>0.267821</td>\n",
       "      <td>0.221779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.543689</td>\n",
       "      <td>0.284402</td>\n",
       "      <td>0.111761</td>\n",
       "      <td>0.264236</td>\n",
       "      <td>0.220133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training Finished. Best Model Saved at ./results/best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Load Model from ./results/best_model\n",
      ">>> Inference Start...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:31<00:00,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Inference Finished. Saved at ./prediction/output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from rouge import Rouge\n",
    "\n",
    "# Transformers & Torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    BartForConditionalGeneration, \n",
    "    BartConfig,\n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import wandb\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Configuration & Seed Setting (설정 및 시드 고정)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 고득점을 위한 하이퍼파라미터 튜닝\n",
    "CONF = {\n",
    "    \"general\": {\n",
    "        \"data_path\": \"./data/\", # 데이터 경로 (수정 필요 시 변경)\n",
    "        \"model_name\": \"digit82/kobart-summarization\",\n",
    "        \"output_dir\": \"./results\",\n",
    "        \"seed\": 42\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"encoder_max_len\": 512, # 대화문은 길 수 있으므로 512 유지\n",
    "        \"decoder_max_len\": 128, # 요약문 길이를 약간 여유있게 설정\n",
    "        # DialogueSum 데이터셋에 등장하는 특수 토큰들\n",
    "        \"special_tokens\": ['#Person1#', '#Person2#', '#Person3#', '#Person4#', '#Person5#', '#Person6#', '#Person7#', '#PhoneNumber#', '#Address#', '#PassportNumber#']\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": 15, # Early Stopping이 있으므로 넉넉하게\n",
    "        \"learning_rate\": 2e-5,  # Fine-tuning에 적합한 LR\n",
    "        \"per_device_train_batch_size\": 32, # GPU 메모리에 맞춰 조절 (VRAM 부족시 16으로 감소)\n",
    "        \"per_device_eval_batch_size\": 32,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr_scheduler_type\": 'cosine',\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"evaluation_strategy\": 'epoch',\n",
    "        \"save_strategy\": 'epoch',\n",
    "        \"save_total_limit\": 3,\n",
    "        \"fp16\": True, # 학습 속도 향상\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"metric_for_best_model\": \"combined_score\", # 커스텀 메트릭 기준\n",
    "        \"greater_is_better\": True,\n",
    "        \"logging_dir\": \"./logs\",\n",
    "        \"logging_strategy\": \"steps\",\n",
    "        \"logging_steps\": 100,\n",
    "        \"predict_with_generate\": True, # 중요: Eval 시 생성 모드 활성화\n",
    "        \"generation_max_length\": 128,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"report_to\": \"none\" # wandb 사용시 \"wandb\"로 변경\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"ckpt_path\": \"./results/checkpoint-best\", # 학습 후 자동 설정됨\n",
    "        \"result_path\": \"./prediction/\",\n",
    "        \"no_repeat_ngram_size\": 3, # 반복되는 문구 억제 (2 또는 3 추천)\n",
    "        \"early_stopping\": True,\n",
    "        \"generate_max_length\": 128,\n",
    "        \"num_beams\": 5, # Beam Search 크기 증가 (정확도 향상)\n",
    "        \"length_penalty\": 1.0, # 길이에 대한 페널티 (0.6~1.2 실험 권장)\n",
    "        \"batch_size\": 32,\n",
    "        \"remove_tokens\": [] # 추후 토크나이저 로드 후 설정\n",
    "    }\n",
    "}\n",
    "\n",
    "seed_everything(CONF['general']['seed'])\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Data Preprocessing (데이터 전처리)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Tokenizer 로드 및 Special Token 추가\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONF['general']['model_name'])\n",
    "special_tokens_dict = {'additional_special_tokens': CONF['tokenizer']['special_tokens']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "CONF['tokenizer']['bos_token'] = tokenizer.bos_token\n",
    "CONF['tokenizer']['eos_token'] = tokenizer.eos_token\n",
    "CONF['inference']['remove_tokens'] = [tokenizer.bos_token, tokenizer.eos_token, tokenizer.pad_token]\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, bos_token, eos_token):\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "\n",
    "    def make_set_as_df(self, file_path, is_train=True):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if is_train:\n",
    "            return df[['fname', 'dialogue', 'summary']]\n",
    "        else:\n",
    "            return df[['fname', 'dialogue']]\n",
    "\n",
    "    def make_input(self, dataset, is_test=False):\n",
    "        if is_test:\n",
    "            encoder_input = dataset['dialogue']\n",
    "            # Test 시에는 Decoder 입력 시작 토큰만 줌\n",
    "            decoder_input = [self.bos_token] * len(dataset['dialogue'])\n",
    "            return encoder_input.tolist(), list(decoder_input)\n",
    "        else:\n",
    "            encoder_input = dataset['dialogue']\n",
    "            # Train 시에는 Summary를 Decoder 입력으로 사용 (Teacher Forcing)\n",
    "            decoder_input = dataset['summary'].apply(lambda x: self.bos_token + str(x))\n",
    "            decoder_output = dataset['summary'].apply(lambda x: str(x) + self.eos_token)\n",
    "            return encoder_input.tolist(), decoder_input.tolist(), decoder_output.tolist()\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels=None, is_test=False, ids=None):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels\n",
    "        self.is_test = is_test\n",
    "        self.ids = ids\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        \n",
    "        # Decoder Input\n",
    "        if 'input_ids' in self.decoder_input:\n",
    "            item['decoder_input_ids'] = self.decoder_input['input_ids'][idx].clone().detach()\n",
    "            item['decoder_attention_mask'] = self.decoder_input['attention_mask'][idx].clone().detach()\n",
    "        else:\n",
    "            # Inference 시점 (List 형태일 수 있음)\n",
    "            # 여기서는 로직상 토크나이징된 결과가 들어오므로 위 조건문이 실행됨\n",
    "            pass\n",
    "\n",
    "        if not self.is_test:\n",
    "            # Labels (pad token은 loss 계산에서 제외하기 위해 -100 처리하는 것이 좋으나, \n",
    "            # 여기서는 tokenizer가 처리한 그대로 사용하고 pad masking은 DataCollator가 보통 처리함.\n",
    "            # 베이스라인 방식을 따르되, labels가 존재하면 추가.\n",
    "            item['labels'] = self.labels['input_ids'][idx].clone().detach()\n",
    "            \n",
    "        if self.ids is not None:\n",
    "            item['ID'] = self.ids[idx]\n",
    "            \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoder_input['input_ids'])\n",
    "\n",
    "def prepare_data(conf, tokenizer, is_train=True):\n",
    "    preprocessor = Preprocess(conf['tokenizer']['bos_token'], conf['tokenizer']['eos_token'])\n",
    "    data_path = conf['general']['data_path']\n",
    "    \n",
    "    if is_train:\n",
    "        train_df = preprocessor.make_set_as_df(os.path.join(data_path, 'train.csv'))\n",
    "        val_df = preprocessor.make_set_as_df(os.path.join(data_path, 'dev.csv'))\n",
    "        \n",
    "        # Train Data\n",
    "        enc_train, dec_in_train, dec_out_train = preprocessor.make_input(train_df)\n",
    "        tokenized_enc_train = tokenizer(enc_train, return_tensors=\"pt\", padding=True, truncation=True, max_length=conf['tokenizer']['encoder_max_len'])\n",
    "        tokenized_dec_in_train = tokenizer(dec_in_train, return_tensors=\"pt\", padding=True, truncation=True, max_length=conf['tokenizer']['decoder_max_len'])\n",
    "        tokenized_dec_out_train = tokenizer(dec_out_train, return_tensors=\"pt\", padding=True, truncation=True, max_length=conf['tokenizer']['decoder_max_len'])\n",
    "        \n",
    "        train_dataset = CustomDataset(tokenized_enc_train, tokenized_dec_in_train, tokenized_dec_out_train)\n",
    "        \n",
    "        # Val Data\n",
    "        enc_val, dec_in_val, dec_out_val = preprocessor.make_input(val_df)\n",
    "        tokenized_enc_val = tokenizer(enc_val, return_tensors=\"pt\", padding=True, truncation=True, max_length=conf['tokenizer']['encoder_max_len'])\n",
    "        tokenized_dec_in_val = tokenizer(dec_in_val, return_tensors=\"pt\", padding=True, truncation=True, max_length=conf['tokenizer']['decoder_max_len'])\n",
    "        tokenized_dec_out_val = tokenizer(dec_out_val, return_tensors=\"pt\", padding=True, truncation=True, max_length=conf['tokenizer']['decoder_max_len'])\n",
    "        \n",
    "        val_dataset = CustomDataset(tokenized_enc_val, tokenized_dec_in_val, tokenized_dec_out_val)\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    else: # Test\n",
    "        test_df = preprocessor.make_set_as_df(os.path.join(data_path, 'test.csv'), is_train=False)\n",
    "        enc_test, dec_in_test = preprocessor.make_input(test_df, is_test=True)\n",
    "        \n",
    "        tokenized_enc_test = tokenizer(enc_test, return_tensors=\"pt\", padding=True, truncation=True, max_length=conf['tokenizer']['encoder_max_len'])\n",
    "        # Test시 Decoder input은 start token만 있으면 됨 (배치 처리를 위해 tokenize)\n",
    "        tokenized_dec_in_test = tokenizer(dec_in_test, return_tensors=\"pt\", padding=True, truncation=True, max_length=conf['tokenizer']['decoder_max_len'])\n",
    "        \n",
    "        test_dataset = CustomDataset(tokenized_enc_test, tokenized_dec_in_test, is_test=True, ids=test_df['fname'].tolist())\n",
    "        return test_df, test_dataset\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. Model Training (모델 학습)\n",
    "# -----------------------------------------------------------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    rouge = Rouge()\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # [수정 1] predictions 처리\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    \n",
    "    # -100을 pad_token_id로 치환\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # [핵심 수정] skip_special_tokens=False로 설정하여 #Person# 토큰 살리기\n",
    "    decoded_preds = tokenizer.batch_decode(predictions.tolist(), skip_special_tokens=False)\n",
    "    decoded_labels = tokenizer.batch_decode(labels.tolist(), skip_special_tokens=False)\n",
    "    \n",
    "    # [추가] 시스템 토큰(BOS, EOS, PAD)만 수동으로 제거\n",
    "    # 모델이 학습한 Special Token인 #Person1# 등은 남겨야 함\n",
    "    remove_tokens = [tokenizer.bos_token, tokenizer.eos_token, tokenizer.pad_token]\n",
    "    \n",
    "    def clean_text(text_list):\n",
    "        cleaned = []\n",
    "        for text in text_list:\n",
    "            for token in remove_tokens:\n",
    "                if token is not None:\n",
    "                    text = text.replace(token, \"\")\n",
    "            cleaned.append(text.strip())\n",
    "        return cleaned\n",
    "\n",
    "    decoded_preds = clean_text(decoded_preds)\n",
    "    decoded_labels = clean_text(decoded_labels)\n",
    "    \n",
    "    # ROUGE 점수 계산\n",
    "    try:\n",
    "        results = rouge.get_scores(decoded_preds, decoded_labels, avg=True)\n",
    "        r1 = results[\"rouge-1\"][\"f\"]\n",
    "        r2 = results[\"rouge-2\"][\"f\"]\n",
    "        rl = results[\"rouge-l\"][\"f\"]\n",
    "        combined_score = (r1 + r2 + rl) / 3\n",
    "        \n",
    "        return {\n",
    "            \"rouge-1\": r1,\n",
    "            \"rouge-2\": r2,\n",
    "            \"rouge-l\": rl,\n",
    "            \"combined_score\": combined_score\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in metrics: {e}\")\n",
    "        return {\"combined_score\": 0.0}\n",
    "\n",
    "\n",
    "def train():\n",
    "    # 데이터 로드\n",
    "    train_dataset, val_dataset = prepare_data(CONF, tokenizer, is_train=True)\n",
    "    \n",
    "    # 모델 로드\n",
    "    model = BartForConditionalGeneration.from_pretrained(CONF['general']['model_name'])\n",
    "    model.resize_token_embeddings(len(tokenizer)) # Special token 추가 반영\n",
    "    model.to(device)\n",
    "    \n",
    "    # Training Arguments\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=CONF['general']['output_dir'],\n",
    "        overwrite_output_dir=CONF['training']['overwrite_output_dir'],\n",
    "        num_train_epochs=CONF['training']['num_train_epochs'],\n",
    "        learning_rate=CONF['training']['learning_rate'],\n",
    "        per_device_train_batch_size=CONF['training']['per_device_train_batch_size'],\n",
    "        per_device_eval_batch_size=CONF['training']['per_device_eval_batch_size'],\n",
    "        warmup_ratio=CONF['training']['warmup_ratio'],\n",
    "        weight_decay=CONF['training']['weight_decay'],\n",
    "        lr_scheduler_type=CONF['training']['lr_scheduler_type'],\n",
    "        optim=CONF['training']['optim'],\n",
    "        evaluation_strategy=CONF['training']['evaluation_strategy'],\n",
    "        save_strategy=CONF['training']['save_strategy'],\n",
    "        save_total_limit=CONF['training']['save_total_limit'],\n",
    "        fp16=CONF['training']['fp16'],\n",
    "        load_best_model_at_end=CONF['training']['load_best_model_at_end'],\n",
    "        metric_for_best_model=CONF['training']['metric_for_best_model'],\n",
    "        greater_is_better=CONF['training']['greater_is_better'],\n",
    "        logging_dir=CONF['training']['logging_dir'],\n",
    "        logging_steps=CONF['training']['logging_steps'],\n",
    "        predict_with_generate=CONF['training']['predict_with_generate'],\n",
    "        generation_max_length=CONF['training']['generation_max_length'],\n",
    "        report_to=CONF['training']['report_to'],\n",
    "        seed=CONF['general']['seed']\n",
    "    )\n",
    "    \n",
    "    # Early Stopping\n",
    "    early_stopping = EarlyStoppingCallback(\n",
    "        early_stopping_patience=CONF['training']['early_stopping_patience']\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    print(\">>> Training Start...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Best Model 저장\n",
    "    best_model_path = os.path.join(CONF['general']['output_dir'], \"best_model\")\n",
    "    trainer.save_model(best_model_path)\n",
    "    print(f\">>> Training Finished. Best Model Saved at {best_model_path}\")\n",
    "    \n",
    "    return best_model_path\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. Inference (추론)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def inference(model_path=None):\n",
    "    if model_path is None:\n",
    "        model_path = os.path.join(CONF['general']['output_dir'], \"best_model\")\n",
    "        \n",
    "    print(f\">>> Load Model from {model_path}\")\n",
    "    \n",
    "    # 모델 로드\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # 데이터 로드\n",
    "    test_df, test_dataset = prepare_data(CONF, tokenizer, is_train=False)\n",
    "    dataloader = DataLoader(test_dataset, batch_size=CONF['inference']['batch_size'], shuffle=False)\n",
    "    \n",
    "    summary_list = []\n",
    "    fname_list = []\n",
    "    \n",
    "    # 시스템 토큰 정의 (제거 대상)\n",
    "    system_tokens = [tokenizer.bos_token, tokenizer.eos_token, tokenizer.pad_token]\n",
    "\n",
    "    print(\">>> Inference Start...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            summary_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                num_beams=CONF['inference']['num_beams'],\n",
    "                max_length=CONF['inference']['generate_max_length'],\n",
    "                no_repeat_ngram_size=CONF['inference']['no_repeat_ngram_size'],\n",
    "                early_stopping=CONF['inference']['early_stopping'],\n",
    "                length_penalty=CONF['inference']['length_penalty']\n",
    "            )\n",
    "            \n",
    "            # [핵심 수정] skip_special_tokens=False로 변경\n",
    "            decoded = tokenizer.batch_decode(summary_ids, skip_special_tokens=False)\n",
    "            \n",
    "            # [추가] 시스템 토큰만 제거하고 #Person# 등은 유지\n",
    "            cleaned_batch = []\n",
    "            for text in decoded:\n",
    "                for token in system_tokens:\n",
    "                    if token is not None:\n",
    "                        text = text.replace(token, \"\")\n",
    "                cleaned_batch.append(text.strip())\n",
    "            \n",
    "            summary_list.extend(cleaned_batch)\n",
    "            fname_list.extend(batch['ID'])\n",
    "            \n",
    "    # 결과 저장\n",
    "    result_path = CONF['inference']['result_path']\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    \n",
    "    output_df = pd.DataFrame({\n",
    "        \"fname\": fname_list,\n",
    "        \"summary\": summary_list\n",
    "    })\n",
    "    \n",
    "    save_file = os.path.join(result_path, \"output.csv\")\n",
    "    output_df.to_csv(save_file, index=False)\n",
    "    print(f\">>> Inference Finished. Saved at {save_file}\")\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. Main Execution\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 학습 실행\n",
    "    best_ckpt = train()\n",
    "    \n",
    "    # 2. 추론 실행\n",
    "    inference(best_ckpt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
