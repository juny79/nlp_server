{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c0f5229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/nlp_server/venv310/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loaded Config ===\n",
      "{'data_path': './data/',\n",
      " 'model_name': 'eenzeenee/t5-base-korean-summarization',\n",
      " 'output_dir': './t5_outputs'}\n",
      "{'bos_token': '',\n",
      " 'decoder_max_len': 80,\n",
      " 'encoder_max_len': 512,\n",
      " 'eos_token': '</s>',\n",
      " 'special_tokens': ['#Person1#',\n",
      "                    '#Person2#',\n",
      "                    '#Person3#',\n",
      "                    '#Person4#',\n",
      "                    '#Person5#',\n",
      "                    '#Person6#',\n",
      "                    '#Person7#',\n",
      "                    '#PhoneNumber#',\n",
      "                    '#Address#',\n",
      "                    '#PassportNumber#'],\n",
      " 't5_prefix': 'summarize: '}\n",
      "{'do_eval': True,\n",
      " 'do_train': True,\n",
      " 'early_stopping_patience': 3,\n",
      " 'early_stopping_threshold': 0.001,\n",
      " 'evaluation_strategy': 'epoch',\n",
      " 'fp16': True,\n",
      " 'generation_max_length': 80,\n",
      " 'gradient_accumulation_steps': 1,\n",
      " 'greater_is_better': True,\n",
      " 'learning_rate': 5e-05,\n",
      " 'load_best_model_at_end': True,\n",
      " 'logging_dir': './logs',\n",
      " 'logging_strategy': 'epoch',\n",
      " 'lr_scheduler_type': 'cosine',\n",
      " 'metric_for_best_model': 'rouge-l',\n",
      " 'num_train_epochs': 10,\n",
      " 'optim': 'adamw_torch',\n",
      " 'overwrite_output_dir': True,\n",
      " 'per_device_eval_batch_size': 16,\n",
      " 'per_device_train_batch_size': 8,\n",
      " 'predict_with_generate': True,\n",
      " 'report_to': 'none',\n",
      " 'save_strategy': 'epoch',\n",
      " 'save_total_limit': 5,\n",
      " 'seed': 42,\n",
      " 'warmup_ratio': 0.1,\n",
      " 'weight_decay': 0.01}\n",
      "{'entity': 'quriquri7', 'name': 't5-base-run1', 'project': 'dialogue-summ-t5'}\n",
      "{'batch_size': 32,\n",
      " 'ckt_path': 'ì¶”ë¡ ì—_ì‚¬ìš©í• _ì²´í¬í¬ì¸íŠ¸_ê²½ë¡œ_ë˜ëŠ”_output_dir',\n",
      " 'early_stopping': True,\n",
      " 'generate_max_length': 80,\n",
      " 'no_repeat_ngram_size': 3,\n",
      " 'num_beams': 5,\n",
      " 'remove_tokens': ['<usr>', '', '</s>', '<pad>'],\n",
      " 'result_path': './prediction_t5/'}\n",
      "\n",
      "=== Train Sample ===\n",
      "             fname                                           dialogue  \\\n",
      "12452  train_12455  #Person1#: ì•ˆë…•í•˜ì„¸ìš”. í˜¹ì‹œ ë§¨ì²´ìŠ¤í„°ì—ì„œ ì˜¤ì‹  Mr. Green ë§ìœ¼ì‹ ê°€ìš”...   \n",
      "12453  train_12456  #Person1#: Mister Ewingì´ ìš°ë¦¬ íšŒì˜ì¥ì— 4ì‹œì— ì˜¤ë¼ê³  í–ˆì§€, ë§...   \n",
      "12454  train_12457  #Person1#: ì˜¤ëŠ˜ ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\\n#Person2#: ì°¨ë¥¼ ë¹Œë¦¬ê³  ì‹¶...   \n",
      "12455  train_12458  #Person1#: ë„ˆ ì˜¤ëŠ˜ ì¢€ ê¸°ë¶„ ì•ˆ ì¢‹ì•„ ë³´ì¸ë‹¤? ë¬´ìŠ¨ ì¼ ìˆì–´?\\n#Pers...   \n",
      "12456  train_12459  #Person1#: ì—„ë§ˆ, ë‚˜ ë‹¤ìŒ ì£¼ í† ìš”ì¼ì— ì´ëª¨ë¶€ë„¤ ê°€ì¡± ë³´ëŸ¬ ê°€ëŠ”ë°, ì˜¤ëŠ˜ ...   \n",
      "\n",
      "                                                 summary     topic  \n",
      "12452  Tan Lingì€ í°ë¨¸ë¦¬ì™€ ìˆ˜ì—¼ì´ íŠ¹ì§•ì¸ Mr. Greenì„ ë§ì´í•˜ì—¬ í˜¸í…”ë¡œ ì•ˆë‚´í•©...     í˜¸í…” ì•ˆë‚´  \n",
      "12453  #Person1#ê³¼ #Person2#ëŠ” Mister Ewingì˜ ìš”ì²­ì— ë”°ë¼ íšŒì˜ì¥...     íšŒì˜ ì¤€ë¹„  \n",
      "12454       #Person2#ëŠ” #Person1#ì˜ ë„ì›€ìœ¼ë¡œ 5ì¼ ë™ì•ˆ ì†Œí˜•ì°¨ë¥¼ ëŒ€ì—¬í•©ë‹ˆë‹¤.     ì°¨ëŸ‰ ëŒ€ì—¬  \n",
      "12455  #Person2#ì˜ ì–´ë¨¸ë‹ˆê°€ ì§ì¥ì„ ìƒìœ¼ì…¨ë‹¤. #Person2#ëŠ” ì–´ë¨¸ë‹ˆê°€ ìš°ìš¸í•´í•˜...    ì‹¤ì§ê³¼ ëŒ€ì²˜  \n",
      "12456  #Person1#ì€ ë‹¤ìŒ ì£¼ í† ìš”ì¼ì— ì´ëª¨ë¶€ë„¤ ê°€ì¡±ì„ ë°©ë¬¸í•˜ê¸° ìœ„í•´ ì§ì„ ì‹¸ì•¼ í•˜ëŠ”...  ê°€ì¡± ë°©ë¬¸ ì¤€ë¹„  \n",
      "\n",
      "=== Dev Sample ===\n",
      "       fname                                           dialogue  \\\n",
      "494  dev_495  #Person1#: ìƒˆí•´ê°€ ë˜ë‹ˆê¹Œ ë‚˜ë„ ìƒˆ ì¶œë°œì„ í•˜ê¸°ë¡œ í–ˆì–´.\\n#Person2#...   \n",
      "495  dev_496  #Person1#: ë„ˆ Joeë‘ ê²°í˜¼í–ˆì§€?\\n#Person2#: Joe? ë¬´ìŠ¨ ë§ì´...   \n",
      "496  dev_497  #Person1#: ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”, ì•„ì¤Œë§ˆ?\\n#Person2#: ì œ ì°¨ì—ì„œ ...   \n",
      "497  dev_498  #Person1#: ì—¬ë³´ì„¸ìš”, ì•„ë§ˆì¡´ ê³ ê° ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\\n#...   \n",
      "498  dev_499  #Person1#: ë²Œì¨ ì—¬ë¦„ì´ ë‹¤ê°€ì˜¤ë‹¤ë‹ˆ ë¯¿ê¸°ì§€ ì•Šì•„. \\n#Person2#: ë§...   \n",
      "\n",
      "                                               summary       topic  \n",
      "494  #Person1#ì€ ìƒˆí•´ì— ë‹´ë°°ë¥¼ ëŠê³  ì»¤ë°ì•„ì›ƒ í•˜ê¸°ë¡œ ê²°ì‹¬í–ˆìŠµë‹ˆë‹¤. #Person...       ìƒˆí•´ ê²°ì‹¬  \n",
      "495  #Person1#ì€ #Person2#ê°€ Joeì™€ ê²°í˜¼í–ˆë‹¤ê³  ìƒê°í•˜ì§€ë§Œ, #Perso...   ì‚¬ë‘ê³¼ ê²°í˜¼ ì˜¤í•´  \n",
      "496  #Person2#ì˜ ì°¨ì—ì„œ ì†Œë¦¬ê°€ ë‚˜ë©°, ë¸Œë ˆì´í¬ ìˆ˜ë¦¬ê°€ í•„ìš”í•œ ìƒí™©ì…ë‹ˆë‹¤. #Pe...  ì°¨ëŸ‰ ì†ŒìŒ ë° ìˆ˜ë¦¬  \n",
      "497  #Person2#ê°€ ì•„ë§ˆì¡´ ê³ ê° ì„œë¹„ìŠ¤ì— ì „í™”í•˜ì—¬ ì•„ë§ˆì¡´ì—ì„œ êµ¬ë§¤í•œ ì±…ì— 53í˜ì´ì§€...    ì±… í˜ì´ì§€ ëˆ„ë½  \n",
      "498  #Person2#ëŠ” ì—¬ë¦„ë°©í•™ ë™ì•ˆ íŒŒí‹°ì—ì„œ ì¼í•˜ëŠ” íšŒì‚¬ì—ì„œ ì¼í•˜ë©°, ì£¼ë¡œ ìŒì‹ ì¤€ë¹„...    ì—¬ë¦„ë°©í•™ ì¼ìë¦¬  \n"
     ]
    }
   ],
   "source": [
    "# 1. ë°ì´í„° ë° í™˜ê²½ì„¤ì • (T5 ê¸°ë°˜ Dialogue Summarization)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl  # ì“°ì§€ ì•Šë”ë¼ë„ requirements ë§ì¶”ê¸°ìš©\n",
    "from rouge import Rouge\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "import wandb  # ì„ íƒ: í•™ìŠµ ë¡œê¹…ìš©\n",
    "\n",
    "# =========================================\n",
    "# 1-1. ê¸°ë³¸ ì„¤ì •: T5 í•œêµ­ì–´ ìš”ì•½ ëª¨ë¸ ì„ íƒ\n",
    "# =========================================\n",
    "MODEL_NAME = \"eenzeenee/t5-base-korean-summarization\"  # T5-base í•œêµ­ì–´ ìš”ì•½ ëª¨ë¸\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# T5 ê³„ì—´ì€ ë³´í†µ bos_tokenì´ ì—†ìœ¼ë¯€ë¡œ None ë°©ì§€\n",
    "bos_token = tokenizer.bos_token if tokenizer.bos_token is not None else \"\"  # decoder input ì•ì— ë¶™ì¼ í† í° (ì—†ì–´ë„ ë¨)\n",
    "eos_token = tokenizer.eos_token if tokenizer.eos_token is not None else \"\"  # ë³´í†µ \"</s>\"\n",
    "\n",
    "# =========================================\n",
    "# 1-2. YAML Config êµ¬ì„±\n",
    "# =========================================\n",
    "config_data = {\n",
    "    \"general\": {\n",
    "        \"data_path\": \"./data/\",           # train.csv, dev.csv, test.csv ìœ„ì¹˜\n",
    "        \"model_name\": MODEL_NAME,          # ì‚¬ìš©í•  T5 ëª¨ë¸\n",
    "        \"output_dir\": \"./t5_outputs\",      # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"encoder_max_len\": 512,            # ëŒ€í™”ê°€ ê½¤ ê¸¸ ìˆ˜ ìˆì–´ì„œ 512 ìœ ì§€\n",
    "        \"decoder_max_len\": 80,             # ìš”ì•½ì€ ìƒëŒ€ì ìœ¼ë¡œ ì§§ê²Œ\n",
    "        \"bos_token\": bos_token,\n",
    "        \"eos_token\": eos_token,\n",
    "        # ëŒ€í™” ë‚´ íŠ¹ìˆ˜ í† í°ì„ ë³´ì¡´í•˜ê¸° ìœ„í•´ special_tokens ë“±ë¡\n",
    "        \"special_tokens\": [\n",
    "            \"#Person1#\", \"#Person2#\", \"#Person3#\",\n",
    "            \"#Person4#\", \"#Person5#\", \"#Person6#\", \"#Person7#\",\n",
    "            \"#PhoneNumber#\", \"#Address#\", \"#PassportNumber#\"\n",
    "        ],\n",
    "        # T5 ìš”ì•½ í”„ë¡¬í”„íŠ¸\n",
    "        \"t5_prefix\": \"summarize: \"\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": 10,            # KoBART 20ep â†’ T5 8~12ep ê¶Œì¥, ì¼ë‹¨ 10\n",
    "        \"learning_rate\": 5e-5,             # ìš”ì•½ íƒœìŠ¤í¬ìš©ìœ¼ë¡œ ìì£¼ ì“°ëŠ” lr\n",
    "        \"per_device_train_batch_size\": 8,  # OOM ë°©ì§€ & ì•ˆì •ì ì¸ í•™ìŠµ\n",
    "        \"per_device_eval_batch_size\": 16,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr_scheduler_type\": \"cosine\",\n",
    "        \"optim\": \"adamw_torch\",\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"save_total_limit\": 5,\n",
    "        \"fp16\": True,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"seed\": 42,\n",
    "        \"logging_dir\": \"./logs\",\n",
    "        \"logging_strategy\": \"epoch\",\n",
    "        \"predict_with_generate\": True,     # ROUGE ê³„ì‚°ì„ ìœ„í•´ generate ì‚¬ìš©\n",
    "        \"generation_max_length\": 80,\n",
    "        \"metric_for_best_model\": \"rouge-l\",  # ROUGE-L ê¸°ì¤€ìœ¼ë¡œ best ckpt ì„ íƒ\n",
    "        \"greater_is_better\": True,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_threshold\": 0.001,\n",
    "        \"report_to\": \"none\",             # \"wandb\" ë˜ëŠ” \"none\"\n",
    "    },\n",
    "    \"wandb\": {\n",
    "        \"entity\": \"quriquri7\",    # ì˜ˆ: \"quriquri7\"\n",
    "        \"project\": \"dialogue-summ-t5\",     # ì›í•˜ëŠ” í”„ë¡œì íŠ¸ëª…\n",
    "        \"name\": \"t5-base-run1\"            # run ì´ë¦„\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"ckt_path\": \"ì¶”ë¡ ì—_ì‚¬ìš©í• _ì²´í¬í¬ì¸íŠ¸_ê²½ë¡œ_ë˜ëŠ”_output_dir\",  # ì˜ˆ: \"t5_outputs/checkpoint-xxxx\"\n",
    "        \"result_path\": \"./prediction_t5/\",\n",
    "        \"no_repeat_ngram_size\": 3,        # ë°˜ë³µ ê°ì†Œ â†’ ROUGE ìƒìŠ¹ì— ë„ì›€\n",
    "        \"early_stopping\": True,\n",
    "        \"generate_max_length\": 80,\n",
    "        \"num_beams\": 5,                   # ë¹” ì„œì¹˜; 4~8 ë²”ìœ„ íŠœë‹ ì¶”ì²œ\n",
    "        \"batch_size\": 32,\n",
    "        \"remove_tokens\": [\n",
    "            \"<usr>\",\n",
    "            bos_token,\n",
    "            eos_token,\n",
    "            tokenizer.pad_token if tokenizer.pad_token is not None else \"\",\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "# =========================================\n",
    "# 1-3. Config ì €ì¥ ë° ë¡œë“œ\n",
    "# =========================================\n",
    "os.makedirs(\"./\", exist_ok=True)\n",
    "config_path = \"./config_t5.yaml\"\n",
    "\n",
    "with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.dump(config_data, f, allow_unicode=True)\n",
    "\n",
    "with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    loaded_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"=== Loaded Config ===\")\n",
    "pprint(loaded_config[\"general\"])\n",
    "pprint(loaded_config[\"tokenizer\"])\n",
    "pprint(loaded_config[\"training\"])\n",
    "pprint(loaded_config[\"wandb\"])\n",
    "pprint(loaded_config[\"inference\"])\n",
    "\n",
    "# =========================================\n",
    "# 1-4. ë°ì´í„° ë¡œë“œ í™•ì¸\n",
    "# =========================================\n",
    "data_path = loaded_config[\"general\"][\"data_path\"]\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "val_df   = pd.read_csv(os.path.join(data_path, \"dev.csv\"))\n",
    "\n",
    "print(\"\\n=== Train Sample ===\")\n",
    "print(train_df.tail())\n",
    "print(\"\\n=== Dev Sample ===\")\n",
    "print(val_df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "416b59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ë°ì´í„° ê°€ê³µ ë° ë°ì´í„°ì…‹ í´ë˜ìŠ¤ êµ¬ì¶• (T5 summarizationìš©)\n",
    "\n",
    "class Preprocess:\n",
    "    \"\"\"\n",
    "    - train/dev:\n",
    "        encoder_input  : \"summarize: \" + dialogue\n",
    "        decoder_input  : (teacher forcingìš©) bos_token + summary  (bos_tokenì´ \"\"ì´ë©´ summary ê·¸ëŒ€ë¡œ)\n",
    "        decoder_output : summary + eos_token\n",
    "    - test:\n",
    "        encoder_input  : \"summarize: \" + dialogue\n",
    "        decoder_input  : bos_token (dummy, T5ëŠ” labelsë§Œ ìˆì–´ë„ ë™ì‘í•˜ì§€ë§Œ êµ¬ì¡° í†µì¼)\n",
    "    \"\"\"\n",
    "    def __init__(self, bos_token: str, eos_token: str, t5_prefix: str = \"summarize: \"):\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.prefix = t5_prefix\n",
    "\n",
    "    @staticmethod\n",
    "    def make_set_as_df(file_path: str, is_train: bool = True):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if is_train:\n",
    "            return df[[\"fname\", \"dialogue\", \"summary\"]]\n",
    "        else:\n",
    "            return df[[\"fname\", \"dialogue\"]]\n",
    "\n",
    "    def make_input(self, dataset: pd.DataFrame, is_test: bool = False):\n",
    "        # encoder ì…ë ¥ì—ëŠ” í•­ìƒ T5 í”„ë¦¬í”½ìŠ¤ë¥¼ ë¶™ì—¬ì¤Œ\n",
    "        encoder_input = dataset[\"dialogue\"].apply(\n",
    "            lambda x: self.prefix + str(x)\n",
    "        )\n",
    "\n",
    "        if is_test:\n",
    "            decoder_input = [self.bos_token] * len(dataset[\"dialogue\"])\n",
    "            return encoder_input.tolist(), decoder_input\n",
    "        else:\n",
    "            # Teacher forcing ìš©\n",
    "            decoder_input = dataset[\"summary\"].apply(\n",
    "                lambda x: self.bos_token + str(x)\n",
    "            )\n",
    "            decoder_output = dataset[\"summary\"].apply(\n",
    "                lambda x: str(x) + self.eos_token\n",
    "            )\n",
    "            return (\n",
    "                encoder_input.tolist(),\n",
    "                decoder_input.tolist(),\n",
    "                decoder_output.tolist(),\n",
    "            )\n",
    "\n",
    "# ============================\n",
    "# Dataset í´ë˜ìŠ¤ë“¤\n",
    "# ============================\n",
    "\n",
    "class DatasetForTrain(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels, length):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels    # dict or tensor\n",
    "        self.length = length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # encoder\n",
    "        item = {\n",
    "            key: val[idx].clone().detach()\n",
    "            for key, val in self.encoder_input.items()\n",
    "        }  # input_ids, attention_mask\n",
    "\n",
    "        # decoder (teacher forcing)\n",
    "        dec = {\n",
    "            key: val[idx].clone().detach()\n",
    "            for key, val in self.decoder_input.items()\n",
    "        }\n",
    "        dec[\"decoder_input_ids\"] = dec[\"input_ids\"]\n",
    "        dec[\"decoder_attention_mask\"] = dec[\"attention_mask\"]\n",
    "        dec.pop(\"input_ids\")\n",
    "        dec.pop(\"attention_mask\")\n",
    "\n",
    "        item.update(dec)\n",
    "        # ğŸ”¥ ì—¬ê¸°ì—ì„œ -100ì´ ì´ë¯¸ ë“¤ì–´ê°„ labels ì‚¬ìš©\n",
    "        item[\"labels\"] = self.labels[\"input_ids\"][idx]  # T5ëŠ” labelsë§Œ ìˆìœ¼ë©´ ë‚´ë¶€ì—ì„œ shift\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "class DatasetForVal(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels, length):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels\n",
    "        self.length = length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: val[idx].clone().detach()\n",
    "            for key, val in self.encoder_input.items()\n",
    "        }\n",
    "        dec = {\n",
    "            key: val[idx].clone().detach()\n",
    "            for key, val in self.decoder_input.items()\n",
    "        }\n",
    "        dec[\"decoder_input_ids\"] = dec[\"input_ids\"]\n",
    "        dec[\"decoder_attention_mask\"] = dec[\"attention_mask\"]\n",
    "        dec.pop(\"input_ids\")\n",
    "        dec.pop(\"attention_mask\")\n",
    "\n",
    "        item.update(dec)\n",
    "        # ğŸ”¥ ì—¬ê¸°ì—ì„œ -100ì´ ì´ë¯¸ ë“¤ì–´ê°„ labels ì‚¬ìš©\n",
    "        item[\"labels\"] = self.labels[\"input_ids\"][idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "class DatasetForInference(Dataset):\n",
    "    def __init__(self, encoder_input, test_id, length):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.test_id = test_id\n",
    "        self.length = length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: val[idx].clone().detach()\n",
    "            for key, val in self.encoder_input.items()\n",
    "        }\n",
    "        item[\"ID\"] = self.test_id[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Train/Val Dataset ìƒì„± í•¨ìˆ˜\n",
    "# ============================\n",
    "\n",
    "def prepare_train_dataset(config, preprocessor: Preprocess, data_path: str, tokenizer):\n",
    "    train_file_path = os.path.join(data_path, \"train.csv\")\n",
    "    val_file_path   = os.path.join(data_path, \"dev.csv\")\n",
    "\n",
    "    train_data = preprocessor.make_set_as_df(train_file_path, is_train=True)\n",
    "    val_data   = preprocessor.make_set_as_df(val_file_path,   is_train=True)\n",
    "\n",
    "    print(\"-\" * 150)\n",
    "    print(f\"[Train dialogue]\\n{train_data['dialogue'][0]}\")\n",
    "    print(f\"[Train summary]\\n{train_data['summary'][0]}\")\n",
    "    print(\"-\" * 150)\n",
    "    print(f\"[Dev dialogue]\\n{val_data['dialogue'][0]}\")\n",
    "    print(f\"[Dev summary]\\n{val_data['summary'][0]}\")\n",
    "\n",
    "    encoder_input_train, decoder_input_train, decoder_output_train = preprocessor.make_input(train_data)\n",
    "    encoder_input_val,   decoder_input_val,   decoder_output_val   = preprocessor.make_input(val_data)\n",
    "\n",
    "    print(\"-\" * 10, \"Load data complete\", \"-\" * 10)\n",
    "\n",
    "    # ----- Train tokenization -----\n",
    "    tokenized_encoder_inputs = tokenizer(\n",
    "        encoder_input_train,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=config[\"tokenizer\"][\"encoder_max_len\"],\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "    tokenized_decoder_inputs = tokenizer(\n",
    "        decoder_input_train,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=config[\"tokenizer\"][\"decoder_max_len\"],\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "    tokenized_decoder_outputs = tokenizer(\n",
    "        decoder_output_train,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=config[\"tokenizer\"][\"decoder_max_len\"],\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "    # ğŸ”¥ [ì¤‘ìš”] labelsì—ì„œ pad_token_idë¥¼ -100ìœ¼ë¡œ ë°”ê¿”ì£¼ê¸°\n",
    "    labels_train = tokenized_decoder_outputs[\"input_ids\"].clone()\n",
    "    labels_train[labels_train == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    train_dataset = DatasetForTrain(\n",
    "        tokenized_encoder_inputs,\n",
    "        tokenized_decoder_inputs,\n",
    "        {\"input_ids\": labels_train},   # â† labels dictë¡œ ê°ì‹¸ì¤Œ\n",
    "        len(encoder_input_train),\n",
    "    )\n",
    "\n",
    "    # ----- Val tokenization -----\n",
    "    val_tokenized_encoder_inputs = tokenizer(\n",
    "        encoder_input_val,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=config[\"tokenizer\"][\"encoder_max_len\"],\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "    val_tokenized_decoder_inputs = tokenizer(\n",
    "        decoder_input_val,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=config[\"tokenizer\"][\"decoder_max_len\"],\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "    val_tokenized_decoder_outputs = tokenizer(\n",
    "        decoder_output_val,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=config[\"tokenizer\"][\"decoder_max_len\"],\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "    labels_val = val_tokenized_decoder_outputs[\"input_ids\"].clone()\n",
    "    labels_val[labels_val == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    val_dataset = DatasetForVal(\n",
    "        val_tokenized_encoder_inputs,\n",
    "        val_tokenized_decoder_inputs,\n",
    "        {\"input_ids\": labels_val},   # â† labels dictë¡œ ê°ì‹¸ì¤Œ\n",
    "        len(encoder_input_val),\n",
    "    )\n",
    "\n",
    "    print(\"-\" * 10, \"Make dataset complete\", \"-\" * 10)\n",
    "    return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "470b0405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Trainer ë° TrainingArguments êµ¬ì¶•í•˜ê¸° (T5 ë²„ì „)\n",
    "\n",
    "def compute_metrics(config, tokenizer, pred):\n",
    "    \"\"\"\n",
    "    - ëŒ€íšŒ í‰ê°€ëŠ” í˜•íƒœì†Œ ë‹¨ìœ„ í† í¬ë‚˜ì´ì¦ˆ í›„ ROUGEë¥¼ ê³„ì‚°í•˜ì§€ë§Œ,\n",
    "      ì—¬ê¸°ì„œëŠ” ë¬¸ì¥ ë‹¨ìœ„ token ê¸°ë°˜ ROUGEë¥¼ ì‚¬ìš©.\n",
    "    - ëŒ€ì‹  remove_tokensë¡œ <usr>, BOS/EOS, PAD ë“±ì€ ì •ë¦¬í•´ ì¤Œ.\n",
    "    \"\"\"\n",
    "    rouge = Rouge()\n",
    "    predictions = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    # -100ì„ pad_token_idë¡œ ë³µêµ¬\n",
    "    predictions[predictions == -100] = tokenizer.pad_token_id\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=False)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=False)\n",
    "\n",
    "    # ë¶ˆí•„ìš” í† í° ì œê±°\n",
    "    remove_tokens = config[\"inference\"][\"remove_tokens\"]\n",
    "    replaced_preds = decoded_preds.copy()\n",
    "    replaced_labels = decoded_labels.copy()\n",
    "\n",
    "    for token in remove_tokens:\n",
    "        if token:\n",
    "            replaced_preds  = [sent.replace(token, \" \") for sent in replaced_preds]\n",
    "            replaced_labels = [sent.replace(token, \" \") for sent in replaced_labels]\n",
    "\n",
    "    # ë””ë²„ê¹…ìš© ìƒ˜í”Œ ì¶œë ¥\n",
    "    print(\"-\" * 150)\n",
    "    print(f\"[PRED 0]: {replaced_preds[0]}\")\n",
    "    print(f\"[GOLD 0]: {replaced_labels[0]}\")\n",
    "    if len(replaced_preds) > 1:\n",
    "        print(\"-\" * 150)\n",
    "        print(f\"[PRED 1]: {replaced_preds[1]}\")\n",
    "        print(f\"[GOLD 1]: {replaced_labels[1]}\")\n",
    "    if len(replaced_preds) > 2:\n",
    "        print(\"-\" * 150)\n",
    "        print(f\"[PRED 2]: {replaced_preds[2]}\")\n",
    "        print(f\"[GOLD 2]: {replaced_labels[2]}\")\n",
    "\n",
    "    # ROUGE ê³„ì‚°\n",
    "    results = rouge.get_scores(replaced_preds, replaced_labels, avg=True)\n",
    "    result = {key: value[\"f\"] for key, value in results.items()}  # f1 score\n",
    "\n",
    "    # Trainerì—ì„œ metric_for_best_model=\"rouge-l\"ë¡œ ì‚¬ìš©\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_trainer_for_train(config, model, tokenizer, train_dataset, val_dataset):\n",
    "    print(\"-\" * 10, \"Make training arguments\", \"-\" * 10)\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config[\"general\"][\"output_dir\"],\n",
    "        overwrite_output_dir=config[\"training\"][\"overwrite_output_dir\"],\n",
    "        num_train_epochs=config[\"training\"][\"num_train_epochs\"],\n",
    "        learning_rate=config[\"training\"][\"learning_rate\"],\n",
    "        per_device_train_batch_size=config[\"training\"][\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=config[\"training\"][\"per_device_eval_batch_size\"],\n",
    "        warmup_ratio=config[\"training\"][\"warmup_ratio\"],\n",
    "        weight_decay=config[\"training\"][\"weight_decay\"],\n",
    "        lr_scheduler_type=config[\"training\"][\"lr_scheduler_type\"],\n",
    "        optim=config[\"training\"][\"optim\"],\n",
    "        gradient_accumulation_steps=config[\"training\"][\"gradient_accumulation_steps\"],\n",
    "        evaluation_strategy=config[\"training\"][\"evaluation_strategy\"],\n",
    "        save_strategy=config[\"training\"][\"save_strategy\"],\n",
    "        save_total_limit=config[\"training\"][\"save_total_limit\"],\n",
    "        fp16=config[\"training\"][\"fp16\"],\n",
    "        load_best_model_at_end=config[\"training\"][\"load_best_model_at_end\"],\n",
    "        seed=config[\"training\"][\"seed\"],\n",
    "        logging_dir=config[\"training\"][\"logging_dir\"],\n",
    "        logging_strategy=config[\"training\"][\"logging_strategy\"],\n",
    "        predict_with_generate=config[\"training\"][\"predict_with_generate\"],\n",
    "        generation_max_length=config[\"training\"][\"generation_max_length\"],\n",
    "        do_train=config[\"training\"][\"do_train\"],\n",
    "        do_eval=config[\"training\"][\"do_eval\"],\n",
    "        report_to=config[\"training\"][\"report_to\"],  # \"wandb\" or \"none\"\n",
    "        metric_for_best_model=config[\"training\"][\"metric_for_best_model\"],\n",
    "        greater_is_better=config[\"training\"][\"greater_is_better\"],\n",
    "        generation_num_beams=config[\"inference\"][\"num_beams\"],\n",
    "        # ì˜µì…˜: label_smoothing_factor=0.1  # transformers ë²„ì „ì— ë”°ë¼ ì‚¬ìš© ê°€ëŠ¥\n",
    "        # ğŸ”¥ wandb ì™„ì „ ë¹„í™œì„±í™”\n",
    "        \n",
    "    )\n",
    "\n",
    "    # ----- wandb ì„¤ì • -----\n",
    "    #USE_WANDB = config[\"training\"][\"report_to\"] == \"wandb\"\n",
    "    #if USE_WANDB:\n",
    "    #    try:\n",
    "    #        wandb.init(\n",
    "    #            entity=config[\"wandb\"][\"entity\"],\n",
    "    #            project=config[\"wandb\"][\"project\"],\n",
    "    #            name=config[\"wandb\"][\"name\"],\n",
    "    #        )\n",
    "    #        os.environ[\"WANDB_LOG_MODEL\"] = \"true\"\n",
    "    #        os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "    #    except Exception as e:\n",
    "    #        print(\"âš ï¸ wandb init ì‹¤íŒ¨, wandb ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤:\", e)\n",
    "    #        os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "    #        config[\"training\"][\"report_to\"] = \"none\"    \n",
    "    #else:\n",
    "    #    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "    #    print(\"ğŸš« wandb ë¹„í™œì„±í™” ìƒíƒœë¡œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    # ----- EarlyStopping -----\n",
    "    early_stop_cb = EarlyStoppingCallback(\n",
    "        early_stopping_patience=config[\"training\"][\"early_stopping_patience\"],\n",
    "        early_stopping_threshold=config[\"training\"][\"early_stopping_threshold\"],\n",
    "    )\n",
    "\n",
    "    print(\"-\" * 10, \"Make trainer\", \"-\" * 10)\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=lambda pred: compute_metrics(config, tokenizer, pred),\n",
    "        callbacks=[early_stop_cb],\n",
    "    )\n",
    "\n",
    "    print(\"-\" * 10, \"Make trainer complete\", \"-\" * 10)\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def load_tokenizer_and_model_for_train(config, device):\n",
    "    print(\"-\" * 10, \"Load tokenizer & T5 model\", \"-\" * 10)\n",
    "    model_name = config[\"general\"][\"model_name\"]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # ëŒ€í™” ë‚´ íŠ¹ìˆ˜ í† í° ì¶”ê°€\n",
    "    special_tokens_dict = {\"additional_special_tokens\": config[\"tokenizer\"][\"special_tokens\"]}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # í† í¬ë‚˜ì´ì € í¬ê¸° ë³€ê²½ ë°˜ì˜\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.to(device)\n",
    "\n",
    "    print(model.config)\n",
    "    print(\"-\" * 10, \"Load tokenizer & model complete\", \"-\" * 10)\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66d3e315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- device : cuda ----------\n",
      "2.1.2+cu118\n",
      "---------- Load tokenizer & T5 model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/nlp_server/venv310/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Config {\n",
      "  \"_name_or_path\": \"eenzeenee/t5-base-korean-summarization\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"max_length\": 128,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50368\n",
      "}\n",
      "\n",
      "---------- Load tokenizer & model complete ----------\n",
      "---------- tokenizer special tokens :  {'eos_token': '</s>', 'unk_token': '<pad>', 'pad_token': '<pad>', 'additional_special_tokens': ['#Person5#', '#PassportNumber#', '#Person2#', '#Person7#', '#Address#', '#Person1#', '#Person6#', '#Person3#', '#PhoneNumber#', '#Person4#']} ----------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[Train dialogue]\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? \n",
      "#Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. \n",
      "#Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. \n",
      "#Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. \n",
      "#Person1#: ìŒ, ì‹¬ê°í•œ ì§ˆë³‘ì„ í”¼í•˜ë ¤ë©´ ë¯¸ë¦¬ ë°œê²¬í•˜ëŠ” ê²Œ ì œì¼ ì¢‹ê±°ë“ ìš”. ë³¸ì¸ì„ ìœ„í•´ì„œë¼ë„ ë§¤ë…„ í•œ ë²ˆì€ ì˜¤ì„¸ìš”. \n",
      "#Person2#: ì•Œê² ìŠµë‹ˆë‹¤. \n",
      "#Person1#: ì—¬ê¸° ì¢€ ë³¼ê¹Œìš”. ëˆˆê³¼ ê·€ëŠ” ê´œì°®ìœ¼ì‹œë„¤ìš”. ê¹Šê²Œ ìˆ¨ í•œ ë²ˆ ì‰¬ì–´ë³´ì„¸ìš”. Mr. Smith, ë‹´ë°° í”¼ìš°ì„¸ìš”? \n",
      "#Person2#: ë„¤. \n",
      "#Person1#: ë‹´ë°°ê°€ íì•”í•˜ê³  ì‹¬ì¥ë³‘ì˜ ì£¼ëœ ì›ì¸ì¸ ê±° ì•„ì‹œì£ ? ëŠìœ¼ì…”ì•¼ í•´ìš”. \n",
      "#Person2#: ìˆ˜ë°± ë²ˆ ì‹œë„í–ˆëŠ”ë°, ë„ì €íˆ ìŠµê´€ì´ ì•ˆ ëŠì–´ì ¸ìš”. \n",
      "#Person1#: ìŒ, ë„ì›€ ë ë§Œí•œ ìˆ˜ì—…ê³¼ ì•½ë¬¼ë“¤ì´ ìˆìŠµë‹ˆë‹¤. ê°€ì‹œê¸° ì „ì— ë” ì •ë³´ë¥¼ ë“œë¦´ê²Œìš”. \n",
      "#Person2#: ë„¤, ê³ ë§™ìŠµë‹ˆë‹¤, ì˜ì‚¬ ì„ ìƒë‹˜.\n",
      "[Train summary]\n",
      "Mr. SmithëŠ” Dr. Hawkinsì—ê²Œ ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ëŸ¬ ì™€ì„œ, ë§¤ë…„ ê²€ì§„ í•„ìš”ì„±ì„ ì•ˆë‚´ë°›ê³  í¡ì—° ìŠµê´€ ê°œì„ ì„ ìœ„í•œ ë„ì›€ì„ ì œì•ˆë°›ì•˜ìŠµë‹ˆë‹¤.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[Dev dialogue]\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì„¸ìš”?\n",
      "#Person2#: ìš”ì¦˜ ìˆ¨ì‰¬ê¸°ê°€ í˜ë“¤ì–´ìš”.\n",
      "#Person1#: ìµœê·¼ì— ê°ê¸°ì— ê±¸ë ¸ë‚˜ìš”?\n",
      "#Person2#: ì•„ë‹ˆìš”, ê°ê¸°ëŠ” ì•ˆ ê±¸ë ¸ì–´ìš”. ìˆ¨ì‰´ ë•Œ ê°€ìŠ´ì´ ë‹µë‹µí•´ìš”.\n",
      "#Person1#: í˜¹ì‹œ ì•Œê³  ìˆëŠ” ì•Œë ˆë¥´ê¸° ìˆìœ¼ì„¸ìš”?\n",
      "#Person2#: ì•„ë‹ˆìš”, íŠ¹ë³„íˆ ì•Œê³  ìˆëŠ” ì•Œë ˆë¥´ê¸°ëŠ” ì—†ì–´ìš”.\n",
      "#Person1#: ì´ê²Œ í•­ìƒ ê·¸ëŸ°ê°€ìš”, ì•„ë‹ˆë©´ ì£¼ë¡œ í™œë™í•  ë•Œ ê·¸ëŸ°ê°€ìš”?\n",
      "#Person2#: ìš´ë™í•  ë•Œ íŠ¹íˆ ë§ì´ ê·¸ë˜ìš”.\n",
      "#Person1#: ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³´ì‹œëŠ” ê²Œ ì¢‹ê² ì–´ìš”.\n",
      "#Person2#: ë„ì™€ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤, ì˜ì‚¬ ì„ ìƒë‹˜.\n",
      "[Dev summary]\n",
      "#Person2#ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2#ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.\n",
      "---------- Load data complete ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/nlp_server/venv310/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Make dataset complete ----------\n",
      "---------- Make training arguments ----------\n",
      "---------- Make trainer ----------\n",
      "---------- Make trainer complete ----------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4951' max='15580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4951/15580 23:25 < 50:18, 3.52 it/s, Epoch 3.18/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.987500</td>\n",
       "      <td>0.004187</td>\n",
       "      <td>0.068461</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.068461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.007544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 0]:  ê°ê°ê¸°ì—                                                                                                                            \n",
      "[GOLD 0]: #Person2#ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2#ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 1]:  JJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJ                                                \n",
      "[GOLD 1]: #Person1#ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 2]:                                                                                                                                 \n",
      "[GOLD 2]: #Person1#ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2#ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1#ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                                        \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 0]:  #Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1#                                                \n",
      "[GOLD 0]: #Person2#ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2#ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 1]:  JJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJ                                                \n",
      "[GOLD 1]: #Person1#ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 2]:  #Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1#                                                \n",
      "[GOLD 2]: #Person1#ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2#ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1#ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                                        \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 0]:  #Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1#                                                \n",
      "[GOLD 0]: #Person2#ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2#ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 1]:  #Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1#                                                \n",
      "[GOLD 1]: #Person1#ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.                                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[PRED 2]:  #Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1##Person1#                                                \n",
      "[GOLD 2]: #Person1#ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2#ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1#ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.                                                                                        \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 69\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGOLD:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# ìœ„ 1ë²ˆ ì…€ì—ì„œ ë§Œë“  loaded_configë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[43mmain_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 34\u001b[0m, in \u001b[0;36mmain_train\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 4-4. Trainer ìƒì„± ë° í•™ìŠµ\u001b[39;00m\n\u001b[1;32m     26\u001b[0m trainer \u001b[38;5;241m=\u001b[39m load_trainer_for_train(\n\u001b[1;32m     27\u001b[0m     config,\n\u001b[1;32m     28\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     val_dataset,\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 34\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 4-5. wandb ì¢…ë£Œ (ì„ íƒ)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#if config[\"training\"][\"report_to\"] == \"wandb\":\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#    wandb.finish()\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# best ëª¨ë¸ì€ TrainingArguments.load_best_model_at_end=Trueì— ì˜í•´ ë¡œë”©ë¨\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… í•™ìŠµ ì™„ë£Œ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/nlp_server/venv310/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nlp_server/venv310/lib/python3.10/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/nlp_server/venv310/lib/python3.10/site-packages/transformers/trainer.py:2734\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2734\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/nlp_server/venv310/lib/python3.10/site-packages/accelerate/accelerator.py:2011\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2011\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2013\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/nlp_server/venv310/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nlp_server/venv310/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4. ëª¨ë¸ í•™ìŠµí•˜ê¸° (Train Loop)\n",
    "\n",
    "def main_train(config):\n",
    "    # 4-1. device ì„¤ì •\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"-\" * 10, f\"device : {device}\", \"-\" * 10)\n",
    "    print(torch.__version__)\n",
    "\n",
    "    # 4-2. T5 ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "    model, tokenizer = load_tokenizer_and_model_for_train(config, device)\n",
    "    print(\"-\" * 10, \"tokenizer special tokens : \", tokenizer.special_tokens_map, \"-\" * 10)\n",
    "\n",
    "    # 4-3. ë°ì´í„°ì…‹ ìƒì„±\n",
    "    preprocessor = Preprocess(\n",
    "        bos_token=config[\"tokenizer\"][\"bos_token\"],\n",
    "        eos_token=config[\"tokenizer\"][\"eos_token\"],\n",
    "        t5_prefix=config[\"tokenizer\"][\"t5_prefix\"],\n",
    "    )\n",
    "    data_path = config[\"general\"][\"data_path\"]\n",
    "\n",
    "    train_dataset, val_dataset = prepare_train_dataset(\n",
    "        config, preprocessor, data_path, tokenizer\n",
    "    )\n",
    "\n",
    "    # 4-4. Trainer ìƒì„± ë° í•™ìŠµ\n",
    "    trainer = load_trainer_for_train(\n",
    "        config,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # 4-5. wandb ì¢…ë£Œ (ì„ íƒ)\n",
    "    #if config[\"training\"][\"report_to\"] == \"wandb\":\n",
    "    #    wandb.finish()\n",
    "\n",
    "    # best ëª¨ë¸ì€ TrainingArguments.load_best_model_at_end=Trueì— ì˜í•´ ë¡œë”©ë¨\n",
    "    print(\"âœ… í•™ìŠµ ì™„ë£Œ\")\n",
    "\n",
    "     # ğŸ” ì—¬ê¸°ì„œë¶€í„° \"dev ìƒ˜í”Œ 1ê°œ ì§ì ‘ ìš”ì•½í•´ë³´ê¸°\" ë””ë²„ê¹… ì½”ë“œ\n",
    "    print(\"\\n=== Dev ìƒ˜í”Œ ìš”ì•½ í…ŒìŠ¤íŠ¸ ===\")\n",
    "    sample = val_df.iloc[0]  # 1ë²ˆ ì…€ì—ì„œ ë¡œë“œí•œ val_df ì‚¬ìš©\n",
    "\n",
    "    text = config[\"tokenizer\"][\"t5_prefix\"] + sample[\"dialogue\"]\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=config[\"tokenizer\"][\"encoder_max_len\"]\n",
    "    ).to(device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=80,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=3,\n",
    "    )\n",
    "\n",
    "    gen = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(\"GEN :\", gen)\n",
    "    print(\"GOLD:\", sample[\"summary\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ìœ„ 1ë²ˆ ì…€ì—ì„œ ë§Œë“  loaded_configë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •\n",
    "    main_train(loaded_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd84c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ëª¨ë¸ ì¶”ë¡ í•˜ê¸° (Inference)\n",
    "\n",
    "# ì¶”ë¡ ì— ì‚¬ìš©í•  checkpoint ì„¤ì • (í•„ìˆ˜!)\n",
    "loaded_config[\"inference\"][\"ckt_path\"] = \"t5_outputs/checkpoint-xxxx\"  # ì‹¤ì œ ckpt ê²½ë¡œë¡œ êµì²´ í•„ìš”\n",
    "\n",
    "\n",
    "def prepare_test_dataset(config, preprocessor: Preprocess, tokenizer):\n",
    "    test_file_path = os.path.join(config[\"general\"][\"data_path\"], \"test.csv\")\n",
    "\n",
    "    test_data = preprocessor.make_set_as_df(test_file_path, is_train=False)\n",
    "    test_id = test_data[\"fname\"]\n",
    "\n",
    "    print(\"-\" * 150)\n",
    "    print(f\"[Test dialogue sample]\\n{test_data['dialogue'][0]}\")\n",
    "    print(\"-\" * 150)\n",
    "\n",
    "    encoder_input_test, decoder_input_test = preprocessor.make_input(\n",
    "        test_data, is_test=True\n",
    "    )\n",
    "\n",
    "    print(\"-\" * 10, \"Load test data complete\", \"-\" * 10)\n",
    "\n",
    "    test_tokenized_encoder_inputs = tokenizer(\n",
    "        encoder_input_test,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=config[\"tokenizer\"][\"encoder_max_len\"],\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "    test_tokenized_decoder_inputs = tokenizer(\n",
    "        decoder_input_test,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=config[\"tokenizer\"][\"decoder_max_len\"],\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "\n",
    "    test_dataset = DatasetForInference(\n",
    "        test_tokenized_encoder_inputs,\n",
    "        test_id,\n",
    "        len(encoder_input_test),\n",
    "    )\n",
    "\n",
    "    print(\"-\" * 10, \"Make test dataset complete\", \"-\" * 10)\n",
    "    return test_data, test_dataset\n",
    "\n",
    "\n",
    "def load_tokenizer_and_model_for_test(config, device):\n",
    "    print(\"-\" * 10, \"Load tokenizer & model for inference\", \"-\" * 10)\n",
    "\n",
    "    model_name = config[\"general\"][\"model_name\"]\n",
    "    ckt_path = config[\"inference\"][\"ckt_path\"]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    special_tokens_dict = {\"additional_special_tokens\": config[\"tokenizer\"][\"special_tokens\"]}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained(ckt_path)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"-\" * 10, \"Load tokenizer & model complete\", \"-\" * 10)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def inference(config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"-\" * 10, f\"device : {device}\", \"-\" * 10)\n",
    "    print(torch.__version__)\n",
    "\n",
    "    model, tokenizer = load_tokenizer_and_model_for_test(config, device)\n",
    "\n",
    "    preprocessor = Preprocess(\n",
    "        bos_token=config[\"tokenizer\"][\"bos_token\"],\n",
    "        eos_token=config[\"tokenizer\"][\"eos_token\"],\n",
    "        t5_prefix=config[\"tokenizer\"][\"t5_prefix\"],\n",
    "    )\n",
    "\n",
    "    test_data, test_dataset = prepare_test_dataset(config, preprocessor, tokenizer)\n",
    "    dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config[\"inference\"][\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    summaries = []\n",
    "    text_ids = []\n",
    "\n",
    "    no_repeat_ngram_size = config[\"inference\"][\"no_repeat_ngram_size\"]\n",
    "    early_stopping = config[\"inference\"][\"early_stopping\"]\n",
    "    max_length = config[\"inference\"][\"generate_max_length\"]\n",
    "    num_beams = config[\"inference\"][\"num_beams\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            text_ids.extend(batch[\"ID\"])\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                early_stopping=early_stopping,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "            )\n",
    "\n",
    "            decoded = tokenizer.batch_decode(\n",
    "                generated_ids,\n",
    "                skip_special_tokens=False,\n",
    "            )\n",
    "            summaries.extend(decoded)\n",
    "\n",
    "    # ë¶ˆí•„ìš” ìŠ¤í˜ì…œ í† í° ì œê±°\n",
    "    remove_tokens = config[\"inference\"][\"remove_tokens\"]\n",
    "    cleaned_summaries = summaries.copy()\n",
    "    for token in remove_tokens:\n",
    "        if token:\n",
    "            cleaned_summaries = [\n",
    "                sent.replace(token, \" \") for sent in cleaned_summaries\n",
    "            ]\n",
    "\n",
    "    output = pd.DataFrame(\n",
    "        {\n",
    "            \"fname\": test_data[\"fname\"],\n",
    "            \"summary\": cleaned_summaries,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    result_path = config[\"inference\"][\"result_path\"]\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    save_path = os.path.join(result_path, \"output_t5.csv\")\n",
    "    output.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… Inference ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_df = inference(loaded_config)\n",
    "    output_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
