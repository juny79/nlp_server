general:
  data_path: "./data/"
  save_dir: "./checkpoints/"
  output_dir: "./checkpoints/"
  model_name: "digit82/kobart-summarization"

tokenizer:
  encoder_max_len: 640
  decoder_max_len: 120

  # KoBART ê¸°ë³¸ í† í° ìœ ì§€
  bos_token: "<s>"
  eos_token: "</s>"
  pad_token: "<pad>"

  # ğŸ”¥ íƒœê·¸ ì†ì‹¤ ë°©ì§€ í•„ìˆ˜
  special_tokens:
    - "#Person1#"
    - "#Person2#"
    - "#Person3#"
    - "#Person4#"
    - "#PhoneNumber#"
    - "#Address#"
    - "#PassportNumber#"

training:
  overwrite_output_dir: true
  num_train_epochs: 15
  learning_rate: 3.0e-05
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4

  # CPU/AMD í™˜ê²½ì—ì„œë„ ì•ˆì •ì ì¸ ì„¤ì •
  gradient_accumulation_steps: 8
  warmup_ratio: 0.1
  weight_decay: 0.01

  evaluation_strategy: "epoch"
  save_strategy: "epoch"
  save_total_limit: 3
  load_best_model_at_end: true

  fp16: false        # CPU/AMD DirectML í™˜ê²½ í˜¸í™˜

  predict_with_generate: true
  generation_max_length: 120

  logging_dir: "./logs"
  logging_strategy: "epoch"

  lr_scheduler_type: "cosine"
  seed: 42

  report_to: "none"   # wandb í™œì„±í™”í•˜ë ¤ë©´ "wandb"

inference:
  batch_size: 8
  generate_max_length: 120
  num_beams: 5
  no_repeat_ngram_size: 3
  length_penalty: 1.1
  early_stopping: true

  remove_tokens:
    - "<usr>"
    - "<s>"
    - "</s>"
    - "<pad>"

  ckt_path: "./checkpoints/"
  result_path: "./prediction/"

solar:
  enable: true
  api_key_env: "SOLAR_API_KEY"
  temperature: 0.2
  max_tokens: 200
  top_p: 0.9
  beam: false

meta_ranker:
  score_weight_rouge: 0.7
  score_weight_pattern: 0.3

normalize:
  min_length: 20
  max_length: 120
  enforce_single_sentence: true
